[
    {
        "id": "2507.23726",
        "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
        "translation": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving",
        "url": "https://arxiv.org/abs/2507.23726",
        "authors": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
        "publish_date": "2025-07-31T17:00:30.000Z",
        "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\nSeed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine Seed-Geometry, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.",
        "github_repo": "https://github.com/ByteDance-Seed/Seed-Prover",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23779",
        "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
        "translation": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
        "url": "https://arxiv.org/abs/2507.23779",
        "authors": "Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo",
        "publish_date": "2025-07-31T17:59:09.000Z",
        "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \"Iron Man\", are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the Phi-Ground model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder 10B parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textbf{43.2} on\nScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\nhttps://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}",
        "github_repo": "https://github.com/zhangmiaosen2000/Phi-Ground",
        "project_page": "https://zhangmiaosen2000.github.io/Phi-Ground/",
        "model_function": ""
    },
    {
        "id": "2507.23277",
        "title": "iLRM: An Iterative Large 3D Reconstruction Model",
        "translation": "iLRM: An Iterative Large 3D Reconstruction Model",
        "url": "https://arxiv.org/abs/2507.23277",
        "authors": "Gyeongjin Kang, Seungtae Nam, Xiangyu Sun, Sameh Khamis, Abdelrahman Mohamed, Eunbyung Park",
        "publish_date": "2025-07-31T06:33:07.000Z",
        "summary": "Feed-forward 3D modeling has emerged as a promising approach for rapid and\nhigh-quality 3D reconstruction. In particular, directly generating explicit 3D\nrepresentations, such as 3D Gaussian splatting, has attracted significant\nattention due to its fast and high-quality rendering, as well as numerous\napplications. However, many state-of-the-art methods, primarily based on\ntransformer architectures, suffer from severe scalability issues because they\nrely on full attention across image tokens from multiple input views, resulting\nin prohibitive computational costs as the number of views or image resolution\nincreases. Toward a scalable and efficient feed-forward 3D reconstruction, we\nintroduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D\nGaussian representations through an iterative refinement mechanism, guided by\nthree core principles: (1) decoupling the scene representation from input-view\nimages to enable compact 3D representations; (2) decomposing fully-attentional\nmulti-view interactions into a two-stage attention scheme to reduce\ncomputational costs; and (3) injecting high-resolution information at every\nlayer to achieve high-fidelity reconstruction. Experimental results on widely\nused datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms\nexisting methods in both reconstruction quality and speed. Notably, iLRM\nexhibits superior scalability, delivering significantly higher reconstruction\nquality under comparable computational cost by efficiently leveraging a larger\nnumber of input views.",
        "github_repo": "https://github.com/Gynjn/iLRM",
        "project_page": "https://gynjn.github.io/iLRM/",
        "model_function": ""
    },
    {
        "id": "2507.22879",
        "title": "RecGPT Technical Report",
        "translation": "RecGPT Technical Report",
        "url": "https://arxiv.org/abs/2507.22879",
        "authors": "Chao Yi, Dian Chen, Gaoyang Guo, Jiakai Tang, Jian Wu, Jing Yu, Sunhao Dai, Wen Chen, Wenjun Yang, Yuning Jiang, Zhujin Gao, Bo Zheng, Chi Li, Dimin Wang, Dixuan Wang, Fan Li, Fan Zhang, Haibin Chen, Haozhuang Liu, Jialin Zhu, Jiamang Wang, Jiawei Wu, Jin Cui, Ju Huang, Kai Zhang, Kan Liu, Lang Tian, Liang Rao, Longbin Li, Lulu Zhao, Mao Zhang, Na He, Peiyang Wang, Qiqi Huang, Tao Luo, Wenbo Su, Xiaoxiao He, Xin Tong, Xu Chen, Xunke Xi, Yang Li, Yaxuan Wu, Yeqiu Yang, Yi Hu, Yinnan Song, Yuchen Li, Yujie Luo, Yujin Yuan, Yuliang Yan, Zhengyang Wang, Zhibo Xiao, Zhixin Ma, Zile Zhou",
        "publish_date": "2025-07-30T17:55:06.000Z",
        "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23682",
        "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action   Models",
        "translation": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action   Models",
        "url": "https://arxiv.org/abs/2507.23682",
        "authors": "Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, Jianyu Chen, Jiang Bian",
        "publish_date": "2025-07-31T15:57:46.000Z",
        "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.",
        "github_repo": "https://github.com/microsoft/villa-x/",
        "project_page": "https://microsoft.github.io/villa-x/",
        "model_function": ""
    },
    {
        "id": "2507.22968",
        "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring   Challenges in Complex Conversations",
        "translation": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring   Challenges in Complex Conversations",
        "url": "https://arxiv.org/abs/2507.22968",
        "authors": "Chengqian Ma, Wei Tao, Yiwen Guo",
        "publish_date": "2025-07-30T17:56:23.000Z",
        "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.",
        "github_repo": "https://github.com/step-out/C3",
        "project_page": "https://step-out.github.io/C3-web/",
        "model_function": ""
    },
    {
        "id": "2507.21509",
        "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language   Models",
        "translation": "Persona Vectors: Monitoring and Controlling Character Traits in Language   Models",
        "url": "https://arxiv.org/abs/2507.21509",
        "authors": "Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey",
        "publish_date": "2025-07-29T05:20:14.000Z",
        "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.",
        "github_repo": "https://github.com/safety-research/persona_vectors/tree/main",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23698",
        "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial   Intelligence in Visuomotor Agents",
        "translation": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial   Intelligence in Visuomotor Agents",
        "url": "https://arxiv.org/abs/2507.23698",
        "authors": "Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang",
        "publish_date": "2025-07-31T16:20:02.000Z",
        "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by 4times and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.",
        "github_repo": "https://github.com/CraftJarvis/ROCKET-3",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23374",
        "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
        "translation": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
        "url": "https://arxiv.org/abs/2507.23374",
        "authors": "Shuangkang Fang, I-Chao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou",
        "publish_date": "2025-07-31T09:43:31.000Z",
        "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21584",
        "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination   Reduction in MLLMs",
        "translation": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination   Reduction in MLLMs",
        "url": "https://arxiv.org/abs/2507.21584",
        "authors": "Kejia Zhang, Keda Tao, Zhiming Luo, Chang Liu, Jiasheng Tang, Huan Wang",
        "publish_date": "2025-07-29T08:39:19.000Z",
        "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.",
        "github_repo": "https://github.com/KejiaZhang-Robust/TARS",
        "project_page": "https://kejiazhang-robust.github.io/tars_web/",
        "model_function": ""
    },
    {
        "id": "2507.20519",
        "title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
        "translation": "AgroBench: Vision-Language Model Benchmark in Agriculture",
        "url": "https://arxiv.org/abs/2507.20519",
        "authors": "Risa Shinoda, Nakamasa Inoue, Hirokatsu Kataoka, Masaki Onishi, Yoshitaka Ushiku",
        "publish_date": "2025-07-28T04:58:29.000Z",
        "summary": "Precise automated understanding of agricultural tasks such as disease\nidentification is essential for sustainable crop production. Recent advances in\nvision-language models (VLMs) are expected to further expand the range of\nagricultural tasks by facilitating human-model interaction through easy,\ntext-based communication. Here, we introduce AgroBench (Agronomist AI\nBenchmark), a benchmark for evaluating VLM models across seven agricultural\ntopics, covering key areas in agricultural engineering and relevant to\nreal-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is\nannotated by expert agronomists. Our AgroBench covers a state-of-the-art range\nof categories, including 203 crop categories and 682 disease categories, to\nthoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal\nthat VLMs have room for improvement in fine-grained identification tasks.\nNotably, in weed identification, most open-source VLMs perform close to random.\nWith our wide range of topics and expert-annotated categories, we analyze the\ntypes of errors made by VLMs and suggest potential pathways for future VLM\ndevelopment. Our dataset and code are available at\nhttps://dahlian00.github.io/AgroBenchPage/ .",
        "github_repo": "https://github.com/dahlian00/AgroBench/tree/main",
        "project_page": "https://dahlian00.github.io/AgroBenchPage/",
        "model_function": ""
    },
    {
        "id": "2507.23632",
        "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network   Perspective",
        "translation": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network   Perspective",
        "url": "https://arxiv.org/abs/2507.23632",
        "authors": "Gabriel Mongaras, Eric C. Larson",
        "publish_date": "2025-07-31T15:10:03.000Z",
        "summary": "Since its introduction, softmax attention has become the backbone of modern\ntransformer architectures due to its expressiveness and scalability across a\nwide range of tasks. However, the main drawback of softmax attention is the\nquadratic memory requirement and computational complexity with respect to the\nsequence length. By replacing the softmax nonlinearity, linear attention and\nsimilar methods have been introduced to avoid the quadratic bottleneck of\nsoftmax attention. Despite these linear forms of attention being derived from\nthe original softmax formulation, they typically lag in terms of downstream\naccuracy. While strong intuition of the softmax nonlinearity on the query and\nkey inner product suggests that it has desirable properties compared to other\nnonlinearities, the question of why this discrepancy exists still remains\nunanswered. This work demonstrates that linear attention is an approximation of\nsoftmax attention by deriving the recurrent form of softmax attention. Using\nthis form, each part of softmax attention can be described in the language of\nrecurrent neural networks (RNNs). Describing softmax attention as an RNN allows\nfor the ablation of the components of softmax attention to understand the\nimportance of each part and how they interact. In this way, our work helps\nexplain why softmax attention is more expressive than its counterparts.",
        "github_repo": "https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23436",
        "title": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for   Culturally Diverse Art Style Classification",
        "translation": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for   Culturally Diverse Art Style Classification",
        "url": "https://arxiv.org/abs/2507.23436",
        "authors": "Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Cosimo Distante, Abdelmalik Taleb-Ahmed",
        "publish_date": "2025-07-31T11:16:00.000Z",
        "summary": "Art style classification remains a formidable challenge in computational\naesthetics due to the scarcity of expertly labeled datasets and the intricate,\noften nonlinear interplay of stylistic elements. While recent dual-teacher\nself-supervised frameworks reduce reliance on labeled data, their linear\nprojection layers and localized focus struggle to model global compositional\ncontext and complex style-feature interactions. We enhance the dual-teacher\nknowledge distillation framework to address these limitations by replacing\nconventional MLP projection and prediction heads with Kolmogorov-Arnold\nNetworks (KANs). Our approach retains complementary guidance from two teacher\nnetworks, one emphasizing localized texture and brushstroke patterns, the other\ncapturing broader stylistic hierarchies while leveraging KANs' spline-based\nactivations to model nonlinear feature correlations with mathematical\nprecision. Experiments on WikiArt and Pandora18k demonstrate that our approach\noutperforms the base dual teacher architecture in Top-1 accuracy. Our findings\nhighlight the importance of KANs in disentangling complex style manifolds,\nleading to better linear probe accuracy than MLP projections.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.14793",
        "title": "Flow Equivariant Recurrent Neural Networks",
        "translation": "Flow Equivariant Recurrent Neural Networks",
        "url": "https://arxiv.org/abs/2507.14793",
        "authors": "T. Anderson Keller",
        "publish_date": "2025-07-20T02:52:21.000Z",
        "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from\none instant to the next. These smooth transformations can be viewed as\ncontinuous symmetries of the environment that we inhabit, defining equivalence\nrelations between stimuli over time. In machine learning, neural network\narchitectures that respect symmetries of their data are called equivariant and\nhave provable benefits in terms of generalization ability and sample\nefficiency. To date, however, equivariance has been considered only for static\ntransformations and feed-forward networks, limiting its applicability to\nsequence models, such as recurrent neural networks (RNNs), and corresponding\ntime-parameterized sequence transformations. In this work, we extend\nequivariant network theory to this regime of `flows' -- one-parameter Lie\nsubgroups capturing natural transformations over time, such as visual motion.\nWe begin by showing that standard RNNs are generally not flow equivariant:\ntheir hidden states fail to transform in a geometrically structured manner for\nmoving stimuli. We then show how flow equivariance can be introduced, and\ndemonstrate that these models significantly outperform their non-equivariant\ncounterparts in terms of training speed, length generalization, and velocity\ngeneralization, on both next step prediction and sequence classification. We\npresent this work as a first step towards building sequence models that respect\nthe time-parameterized symmetries which govern the world around us.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23404",
        "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring",
        "translation": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring",
        "url": "https://arxiv.org/abs/2507.23404",
        "authors": "Salah Eddine Bekhouche, Azeddine Benlamoudi, Yazid Bounab, Fadi Dornaika, Abdenour Hadid",
        "publish_date": "2025-07-31T10:18:28.000Z",
        "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat https://github.com/Bekhouche/APR{GitHub}.",
        "github_repo": "https://github.com/Bekhouche/APR",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.23257",
        "title": "Efficient Machine Unlearning via Influence Approximation",
        "translation": "Efficient Machine Unlearning via Influence Approximation",
        "url": "https://arxiv.org/abs/2507.23257",
        "authors": "Jiawei Liu, Chenwang Wu, Defu Lian, Enhong Chen",
        "publish_date": "2025-07-31T05:34:27.000Z",
        "summary": "Due to growing privacy concerns, machine unlearning, which aims at enabling\nmachine learning models to ``forget\" specific training data, has received\nincreasing attention. Among existing methods, influence-based unlearning has\nemerged as a prominent approach due to its ability to estimate the impact of\nindividual training samples on model parameters without retraining. However,\nthis approach suffers from prohibitive computational overhead arising from the\nnecessity to compute the Hessian matrix and its inverse across all training\nsamples and parameters, rendering it impractical for large-scale models and\nscenarios involving frequent data deletion requests. This highlights the\ndifficulty of forgetting. Inspired by cognitive science, which suggests that\nmemorizing is easier than forgetting, this paper establishes a theoretical link\nbetween memorizing (incremental learning) and forgetting (unlearning). This\nconnection allows machine unlearning to be addressed from the perspective of\nincremental learning. Unlike the time-consuming Hessian computations in\nunlearning (forgetting), incremental learning (memorizing) typically relies on\nmore efficient gradient optimization, which supports the aforementioned\ncognitive theory. Based on this connection, we introduce the Influence\nApproximation Unlearning (IAU) algorithm for efficient machine unlearning from\nthe incremental perspective. Extensive empirical evaluations demonstrate that\nIAU achieves a superior balance among removal guarantee, unlearning efficiency,\nand comparable model utility, while outperforming state-of-the-art methods\nacross diverse datasets and model architectures. Our code is available at\nhttps://github.com/Lolo1222/IAU.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    }
]