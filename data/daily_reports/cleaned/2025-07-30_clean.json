[
    {
        "id": "2507.21809",
        "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D   Worlds from Words or Pixels",
        "translation": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D   Worlds from Words or Pixels",
        "url": "https://arxiv.org/abs/2507.21809",
        "authors": "HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, Yihang Lian, Yulin Tsai, Lifu Wang, Sicong Liu, Puhua Jiang, Xianghui Yang, Dongyuan Guo, Yixuan Tang, Xinyue Mao, Jiaao Yu, Junlin Yu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Chao Zhang, Yonghao Tan, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Minghui Chen, Zhan Li, Wangchen Qin, Lei Wang, Yifu Sun, Lin Niu, Xiang Yuan, Xiaofeng Yang, Yingping He, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Tian Liu, Peng Chen, Di Wang, Yuhong Liu, Linus, Jie Jiang, Tengfei Wang, Chunchao Guo",
        "publish_date": "2025-07-29T13:43:35.000Z",
        "summary": "Creating immersive and playable 3D worlds from texts or images remains a\nfundamental challenge in computer vision and graphics. Existing world\ngeneration approaches typically fall into two categories: video-based methods\nthat offer rich diversity but lack 3D consistency and rendering efficiency, and\n3D-based methods that provide geometric consistency but struggle with limited\ntraining data and memory-inefficient representations. To address these\nlimitations, we present HunyuanWorld 1.0, a novel framework that combines the\nbest of both worlds for generating immersive, explorable, and interactive 3D\nscenes from text and image conditions. Our approach features three key\nadvantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2)\nmesh export capabilities for seamless compatibility with existing computer\ngraphics pipelines; 3) disentangled object representations for augmented\ninteractivity. The core of our framework is a semantically layered 3D mesh\nrepresentation that leverages panoramic images as 360{\\deg} world proxies for\nsemantic-aware world decomposition and reconstruction, enabling the generation\nof diverse 3D worlds. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in generating coherent, explorable, and\ninteractive 3D worlds while enabling versatile applications in virtual reality,\nphysical simulation, game development, and interactive content creation.",
        "github_repo": "https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0",
        "project_page": "https://3d-models.hunyuan.tencent.com/world/",
        "model_function": ""
    },
    {
        "id": "2507.22058",
        "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image   Generative Models Great Again",
        "translation": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image   Generative Models Great Again",
        "url": "https://arxiv.org/abs/2507.22058",
        "authors": "Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, Jie Jiang",
        "publish_date": "2025-07-29T17:59:04.000Z",
        "summary": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.",
        "github_repo": "https://github.com/X-Omni-Team/X-Omni",
        "project_page": "https://x-omni-team.github.io",
        "model_function": ""
    },
    {
        "id": "2507.21990",
        "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical   Knowledge",
        "translation": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical   Knowledge",
        "url": "https://arxiv.org/abs/2507.21990",
        "authors": "Zihan Zhao, Bo Chen, Ziping Wan, Lu Chen, Xuanze Lin, Shiyang Yu, Situo Zhang, Da Ma, Zichen Zhu, Danyang Zhang, Huayang Wang, Zhongyang Dai, Liyang Wen, Xin Chen, Kai Yu",
        "publish_date": "2025-07-29T16:40:49.000Z",
        "summary": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves state-of-the-art performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.14111",
        "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement   Learning",
        "translation": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement   Learning",
        "url": "https://arxiv.org/abs/2507.14111",
        "authors": "Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum",
        "publish_date": "2025-07-18T17:43:56.000Z",
        "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20254",
        "title": "MIRepNet: A Pipeline and Foundation Model for EEG-Based Motor Imagery   Classification",
        "translation": "MIRepNet: A Pipeline and Foundation Model for EEG-Based Motor Imagery   Classification",
        "url": "https://arxiv.org/abs/2507.20254",
        "authors": "Dingkun Liu, Zhu Chen, Jingwei Luo, Shijie Lian, Dongrui Wu",
        "publish_date": "2025-07-27T12:54:42.000Z",
        "summary": "Brain-computer interfaces (BCIs) enable direct communication between the\nbrain and external devices. Recent EEG foundation models aim to learn\ngeneralized representations across diverse BCI paradigms. However, these\napproaches overlook fundamental paradigm-specific neurophysiological\ndistinctions, limiting their generalization ability. Importantly, in practical\nBCI deployments, the specific paradigm such as motor imagery (MI) for stroke\nrehabilitation or assistive robotics, is generally determined prior to data\nacquisition. This paper proposes MIRepNet, the first EEG foundation model\ntailored for the MI paradigm. MIRepNet comprises a high-quality EEG\npreprocessing pipeline incorporating a neurophysiologically-informed channel\ntemplate, adaptable to EEG headsets with arbitrary electrode configurations.\nFurthermore, we introduce a hybrid pretraining strategy that combines\nself-supervised masked token reconstruction and supervised MI classification,\nfacilitating rapid adaptation and accurate decoding on novel downstream MI\ntasks with fewer than 30 trials per class. Extensive evaluations across five\npublic MI datasets demonstrated that MIRepNet consistently achieved\nstate-of-the-art performance, significantly outperforming both specialized and\ngeneralized EEG models. Our code will be available on\nGitHubhttps://github.com/staraink/MIRepNet.",
        "github_repo": "https://github.com/staraink/MIRepNet",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21183",
        "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
        "translation": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
        "url": "https://arxiv.org/abs/2507.21183",
        "authors": "Guangchen Lan, Sipeng Zhang, Tianle Wang, Yuwei Zhang, Daoan Zhang, Xinpeng Wei, Xiaoman Pan, Hongming Zhang, Dong-Jun Han, Christopher G. Brinton",
        "publish_date": "2025-07-27T05:26:50.000Z",
        "summary": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20240",
        "title": "AnimalClue: Recognizing Animals by their Traces",
        "translation": "AnimalClue: Recognizing Animals by their Traces",
        "url": "https://arxiv.org/abs/2507.20240",
        "authors": "Risa Shinoda, Nakamasa Inoue, Iro Laina, Christian Rupprecht, Hirokatsu Kataoka",
        "publish_date": "2025-07-27T11:48:03.000Z",
        "summary": "Wildlife observation plays an important role in biodiversity conservation,\nnecessitating robust methodologies for monitoring wildlife populations and\ninterspecies interactions. Recent advances in computer vision have\nsignificantly contributed to automating fundamental wildlife observation tasks,\nsuch as animal detection and species identification. However, accurately\nidentifying species from indirect evidence like footprints and feces remains\nrelatively underexplored, despite its importance in contributing to wildlife\nmonitoring. To bridge this gap, we introduce AnimalClue, the first large-scale\ndataset for species identification from images of indirect evidence. Our\ndataset consists of 159,605 bounding boxes encompassing five categories of\nindirect clues: footprints, feces, eggs, bones, and feathers. It covers 968\nspecies, 200 families, and 65 orders. Each image is annotated with\nspecies-level labels, bounding boxes or segmentation masks, and fine-grained\ntrait information, including activity patterns and habitat preferences. Unlike\nexisting datasets primarily focused on direct visual features (e.g., animal\nappearances), AnimalClue presents unique challenges for classification,\ndetection, and instance segmentation tasks due to the need for recognizing more\ndetailed and subtle visual features. In our experiments, we extensively\nevaluate representative vision models and identify key challenges in animal\nidentification from their traces. Our dataset and code are available at\nhttps://dahlian00.github.io/AnimalCluePage/",
        "github_repo": "https://github.com/dahlian00/AnimalClue",
        "project_page": "https://dahlian00.github.io/AnimalCluePage/",
        "model_function": ""
    },
    {
        "id": "2507.22061",
        "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
        "translation": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
        "url": "https://arxiv.org/abs/2507.22061",
        "authors": "Kaining Ying, Hengrui Hu, Henghui Ding",
        "publish_date": "2025-07-29T17:59:35.000Z",
        "summary": "This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21364",
        "title": "Evaluating Deep Learning Models for African Wildlife Image   Classification: From DenseNet to Vision Transformers",
        "translation": "Evaluating Deep Learning Models for African Wildlife Image   Classification: From DenseNet to Vision Transformers",
        "url": "https://arxiv.org/abs/2507.21364",
        "authors": "Lukman Jibril Aliyu, Umar Sani Muhammad, Bilqisu Ismail, Nasiru Muhammad, Almustapha A Wakili, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, Mustapha Abdullahi",
        "publish_date": "2025-07-28T22:18:13.000Z",
        "summary": "Wildlife populations in Africa face severe threats, with vertebrate numbers\ndeclining by over 65% in the past five decades. In response, image\nclassification using deep learning has emerged as a promising tool for\nbiodiversity monitoring and conservation. This paper presents a comparative\nstudy of deep learning models for automatically classifying African wildlife\nimages, focusing on transfer learning with frozen feature extractors. Using a\npublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; we\nevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and\nVision Transformer ViT-H/14. DenseNet-201 achieved the best performance among\nconvolutional networks (67% accuracy), while ViT-H/14 achieved the highest\noverall accuracy (99%), but with significantly higher computational cost,\nraising deployment concerns. Our experiments highlight the trade-offs between\naccuracy, resource requirements, and deployability. The best-performing CNN\n(DenseNet-201) was integrated into a HF Mirror Gradio Space for real-time\nfield use, demonstrating the feasibility of deploying lightweight models in\nconservation settings. This work contributes to African-grounded AI research by\noffering practical insights into model selection, dataset preparation, and\nresponsible deployment of deep learning tools for wildlife conservation.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21503",
        "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via   Unanswerable Visual Questions",
        "translation": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via   Unanswerable Visual Questions",
        "url": "https://arxiv.org/abs/2507.21503",
        "authors": "Yanxu Zhu, Shitong Duan, Xiangxu Zhang, Jitao Sang, Peng Zhang, Tun Lu, Xiao Zhou, Jing Yao, Xiaoyuan Yi, Xing Xie",
        "publish_date": "2025-07-29T04:55:49.000Z",
        "summary": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable\nadvancements in vision-language tasks, yet produce potentially harmful or\nuntrustworthy content. Despite substantial work investigating the\ntrustworthiness of language models, MMLMs' capability to act honestly,\nespecially when faced with visually unanswerable questions, remains largely\nunderexplored. This work presents the first systematic assessment of honesty\nbehaviors across various MLLMs. We ground honesty in models' response behaviors\nto unanswerable visual questions, define four representative types of such\nquestions, and construct MoHoBench, a large-scale MMLM honest benchmark,\nconsisting of 12k+ visual question samples, whose quality is guaranteed by\nmulti-stage filtering and human verification. Using MoHoBench, we benchmarked\nthe honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our\nfindings show that: (1) most models fail to appropriately refuse to answer when\nnecessary, and (2) MMLMs' honesty is not solely a language modeling issue, but\nis deeply influenced by visual information, necessitating the development of\ndedicated methods for multimodal honesty alignment. Therefore, we implemented\ninitial alignment methods using supervised and preference learning to improve\nhonesty behavior, providing a foundation for future work on trustworthy MLLMs.\nOur data and code can be found at https://github.com/DSTTSD/MoHoBench.",
        "github_repo": "https://github.com/DSTTSD/MoHoBench",
        "project_page": "",
        "model_function": ""
    }
]