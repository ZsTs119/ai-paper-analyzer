{
  "paper_id": "2507.16806",
  "paper_title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
  "cache_time": 1754308520.0067291,
  "result": {
    "id": "2507.16806",
    "title_en": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
    "title_zh": "超越二元奖励：训练语言模型对其不确定性进行推理",
    "url": "https://arxiv.org/abs/2507.16806",
    "authors": "Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas",
    "publish_date": "2025-07-22",
    "summary_en": "When language models (LMs) are trained via reinforcement learning (RL) to\ngenerate natural language \"reasoning chains\", their performance improves on a\nvariety of difficult question answering tasks. Today, almost all successful\napplications of RL for reasoning use binary reward functions that evaluate the\ncorrectness of LM outputs. Because such reward functions do not penalize\nguessing or low-confidence outputs, they often have the unintended side-effect\nof degrading calibration and increasing the rate at which LMs generate\nincorrect responses (or \"hallucinate\") in other problem domains. This paper\ndescribes RLCR (Reinforcement Learning with Calibration Rewards), an approach\nto training reasoning models that jointly improves accuracy and calibrated\nconfidence estimation. During RLCR, LMs generate both predictions and numerical\nconfidence estimates after reasoning. They are trained to optimize a reward\nfunction that augments a binary correctness score with a Brier score -- a\nscoring rule for confidence estimates that incentivizes calibrated prediction.\nWe first prove that this reward function (or any analogous reward function that\nuses a bounded, proper scoring rule) yields models whose predictions are both\naccurate and well-calibrated. We next show that across diverse datasets, RLCR\nsubstantially improves calibration with no loss in accuracy, on both in-domain\nand out-of-domain evaluations -- outperforming both ordinary RL training and\nclassifiers trained to assign post-hoc confidence scores. While ordinary RL\nhurts calibration, RLCR improves it. Finally, we demonstrate that verbalized\nconfidence can be leveraged at test time to improve accuracy and calibration\nvia confidence-weighted scaling methods. Our results show that explicitly\noptimizing for calibration can produce more generally reliable reasoning\nmodels.",
    "summary_zh": "当语言模型(LMs)通过强化学习(RL)训练以生成自然语言\"推理链\"时，它们在各种困难的问答任务上表现更好。如今，几乎所有用于推理的成功RL应用都使用二元奖励函数来评估LM输出的正确性。由于这类奖励函数不会惩罚猜测或低置信度的输出，它们常常会产生意外的副作用，降低校准能力，并增加LM在其他问题领域中生成不正确响应(或\"产生幻觉\")的速率。本文描述了RLCR(带校准奖励的强化学习)，这是一种训练推理模型的方法，可以同时提高准确性和校准置信度估计。在RLCR过程中，LM在推理后生成预测和数值置信度估计。它们被训练以优化奖励函数，该函数将二元正确性分数与Brier分数(一种置信度估计的评分规则)相结合，该规则激励校准预测。我们首先证明，这种奖励函数(或任何使用有界、适当评分规则的类似奖励函数)产生的模型，其预测既准确又具有良好的校准性。接下来，我们展示在各种数据集上，RLCR在领域内和领域外评估中显著提高了校准能力，同时没有损失准确性，优于普通RL训练和分配事后置信度分数的分类器。虽然普通RL会损害校准能力，但RLCR却能改善它。最后，我们证明在测试时可以利用语言化的置信度，通过置信度加权缩放方法来提高准确性和校准能力。我们的结果表明，明确优化校准可以产生更普遍可靠的推理模型。",
    "github_repo": "暂无",
    "project_page": "暂无",
    "model_function": "训练语言模型生成带有校准置信度估计的推理链，提高预测准确性和可靠性",
    "analysis_time": "2025-08-04T19:55:20.006729"
  }
}