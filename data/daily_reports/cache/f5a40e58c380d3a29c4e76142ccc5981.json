{
  "paper_id": "2507.19399",
  "paper_title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
  "cache_time": 1754308527.3691313,
  "result": {
    "id": "2507.19399",
    "title_en": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
    "title_zh": "在CIRCLE中运行？一个针对LLM代码解释器安全性的简单基准测试",
    "url": "https://arxiv.org/abs/2507.19399",
    "authors": "Gabriel Chua",
    "publish_date": "2025-07-25",
    "summary_en": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.",
    "summary_zh": "随着大型语言模型(LLMs)越来越多地集成原生代码解释器，它们能够实现强大的实时执行能力，大大扩展了它们的实用性。然而，这种集成引入了潜在的系统性网络安全威胁，这些威胁与基于提示的漏洞有根本不同。为了系统性地评估这些特定于解释器的风险，我们提出了CIRCLE（针对LLM漏洞的代码解释器弹性检查），这是一个包含1260个提示的简单基准测试，这些提示针对CPU、内存和磁盘资源耗尽。每个风险类别都包含明确的恶意(\"直接\")和看似良性(\"间接\")的提示变体。我们的自动化评估框架不仅评估LLM是否拒绝或生成有风险的代码，还在解释器环境中执行生成的代码，以评估代码的正确性、LLM为使代码安全所做的简化或执行超时。评估了OpenAI和Google的7个商业可用模型，我们发现存在显著且不一致的漏洞。例如，评估显示，即使在提供商内部也存在巨大差异——OpenAI的o4-mini正确拒绝风险请求的比例为7.1%，明显高于GPT-4.1的0.5%。结果特别强调，间接的、社会工程学的提示显著削弱了模型的防御能力。这突显了对特定于解释器的网络安全基准测试、专门的缓解工具（例如护栏）以及明确的行业标准的迫切需求，以指导LLM解释器集成的安全和负责任的部署。基准测试数据集和评估代码已公开发布，以促进进一步的研究。",
    "github_repo": "暂无",
    "project_page": "暂无",
    "model_function": "评估LLM代码解释器防御资源耗尽攻击的能力，包括直接和间接提示变体。",
    "analysis_time": "2025-08-04T19:55:27.369131"
  }
}