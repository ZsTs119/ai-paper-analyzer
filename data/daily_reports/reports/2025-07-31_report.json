[
    {
        "id": "2507.22607",
        "title_en": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced   Multimodal Reasoning",
        "title_zh": "VL-Cogito：面向高级多模态推理的渐进式课程强化学习",
        "url": "https://arxiv.org/abs/2507.22607",
        "authors": "Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, Hao Zhang, Yu Rong",
        "publish_date": "2025-07-30",
        "summary_en": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.",
        "summary_zh": "强化学习已被证明在提升大型语言模型的推理能力方面具有有效性。最近的研究工作已逐步将这一范式扩展到多模态推理任务中。由于多模态任务固有的复杂性和多样性，特别是在语义内容和问题表述方面，现有模型通常在不同领域和难度水平上表现出不稳定的表现。为解决这些局限性，我们提出了VL-Cogito，这是一个通过新颖的多阶段渐进式课程强化学习(PCuRL)框架训练的高级多模态推理模型。PCuRL系统性地引导模型逐步完成难度递增的任务，显著提高了其在多样化多模态环境中的推理能力。该框架引入了两个关键创新：(1)在线难度软加权机制，在连续的RL训练阶段动态调整训练难度；(2)动态长度奖励机制，鼓励模型根据任务复杂度自适应调节其推理路径长度，从而平衡推理效率与正确性。实验评估表明，VL-Cogito在跨越数学、科学、逻辑和通用理解等主流多模态基准测试中，持续匹配或超越现有面向推理的模型，验证了我们方法的有效性。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "通过渐进式课程强化学习提升多模态推理能力，平衡推理效率与正确性，适用于数学、科学、逻辑等多领域任务",
        "analysis_time": "2025-08-04T19:06:08.149686"
    },
    {
        "id": "2507.21493",
        "title_en": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
        "title_zh": "BANG：通过生成式爆炸动力学分割三维资产",
        "url": "https://arxiv.org/abs/2507.21493",
        "authors": "Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, Jingyi Yu",
        "publish_date": "2025-07-29",
        "summary_en": "3D creation has always been a unique human strength, driven by our ability to\ndeconstruct and reassemble objects using our eyes, mind and hand. However,\ncurrent 3D design tools struggle to replicate this natural process, requiring\nconsiderable artistic expertise and manual labor. This paper introduces BANG, a\nnovel generative approach that bridges 3D generation and reasoning, allowing\nfor intuitive and flexible part-level decomposition of 3D objects. At the heart\nof BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of\nexploded states for an input geometry, progressively separating parts while\npreserving their geometric and semantic coherence.\n  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned\nfor exploded dynamics with a lightweight exploded view adapter, allowing\nprecise control over the decomposition process. It also incorporates a temporal\nattention module to ensure smooth transitions and consistency across time. BANG\nenhances control with spatial prompts, such as bounding boxes and surface\nregions, enabling users to specify which parts to decompose and how. This\ninteraction can be extended with multimodal models like GPT-4, enabling\n2D-to-3D manipulations for more intuitive and creative workflows.\n  The capabilities of BANG extend to generating detailed part-level geometry,\nassociating parts with functional descriptions, and facilitating\ncomponent-aware 3D creation and manufacturing workflows. Additionally, BANG\noffers applications in 3D printing, where separable parts are generated for\neasy printing and reassembly. In essence, BANG enables seamless transformation\nfrom imaginative concepts to detailed 3D assets, offering a new perspective on\ncreation that resonates with human intuition.",
        "summary_zh": "三维创作一直是一种独特的人类优势，源于我们能够使用眼睛、心灵和手来解构和重新组装物体的能力。然而，当前的三维设计工具难以复制这一自然过程，需要相当的艺术专业知识和人工劳动。本文介绍了BANG，一种新颖的生成式方法，它连接了三维生成和推理，允许对三维对象进行直观且灵活的部分级分解。BANG的核心是\"生成式爆炸动力学\"，它为输入几何体创建一系列平滑的爆炸状态，在保持几何和语义一致性的同时逐步分离各个部分。BANG利用预训练的大规模潜在扩散模型，通过轻量级的爆炸视图适配器针对爆炸动力学进行了微调，从而能够精确控制分解过程。它还集成了一个时间注意力模块，确保时间上的平滑过渡和一致性。BANG通过空间提示（如边界框和表面区域）增强控制，使用户能够指定要分解的部分以及如何分解。这种交互可以通过GPT-4等多模态模型进行扩展，实现2D到3D的操作，以提供更直观和创造性的工作流程。BANG的能力扩展到生成详细的部分级几何体，将部分与功能描述相关联，并促进组件感知的三维创作和制造工作流程。此外，BANG在三维打印方面也有应用，可生成可分离的部分以便于打印和重新组装。本质上，BANG实现了从想象概念到详细三维资产的无缝转换，提供了一种与人类直觉共鸣的创作新视角。",
        "github_repo": "暂无",
        "project_page": "https://sites.google.com/view/bang7355608",
        "model_function": "通过生成式爆炸动力学实现三维资产的部分级分解，支持精确控制和创造性工作流程",
        "analysis_time": "2025-08-04T19:06:10.159608"
    },
    {
        "id": "2507.22827",
        "title_en": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End   Automation via Modular Multimodal Agents",
        "title_zh": "ScreenCoder: 通过模块化多模态代理推进前端自动化中的视觉到代码生成",
        "url": "https://arxiv.org/abs/2507.22827",
        "authors": "Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, Xiangyu Yue",
        "publish_date": "2025-07-30",
        "summary_en": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
        "summary_zh": "将用户界面(UI)设计自动转换为前端代码，对于加速软件开发和民主化设计工作流程具有巨大潜力。尽管最近的大型语言模型(LLMs)在文本到代码生成方面已经取得了进展，但许多现有方法仅依赖自然语言提示，限制了它们在捕获空间布局和视觉设计意图方面的有效性。相比之下，UI开发在实践中本质上是多模态的，通常从视觉草图或模型开始。为了解决这一差距，我们引入了一个模块化多代理框架，它通过三个可解释的阶段执行UI到代码的生成：基础、规划和生成。基础代理使用视觉语言模型检测和标记UI组件，规划代理使用前端工程先验知识构建分层布局，生成代理通过自适应提示合成产生HTML/CSS代码。这种设计比端到端的黑盒方法提高了鲁棒性、可解释性和保真度。此外，我们将框架扩展为一个可扩展的数据引擎，自动生成大规模的图像-代码对。使用这些合成示例，我们对开源VLM进行了微调和强化，在UI理解和代码质量方面取得了显著提升。大量实验表明，我们的方法在布局准确性、结构连贯性和代码正确性方面达到了最先进的性能。我们的代码已在https://github.com/leigest519/ScreenCoder公开提供。",
        "github_repo": "https://github.com/leigest519/ScreenCoder",
        "project_page": "https://hf-mirror.com/spaces/Jimmyzheng-10/ScreenCoder",
        "model_function": "通过模块化多代理框架将UI设计自动转换为前端代码，提高布局准确性和代码质量。",
        "analysis_time": "2025-08-04T19:06:11.697897"
    },
    {
        "id": "2507.22062",
        "title_en": "MetaCLIP 2: A Worldwide Scaling Recipe",
        "title_zh": "MetaCLIP 2: 全球扩展配方",
        "url": "https://arxiv.org/abs/2507.22062",
        "authors": "Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu",
        "publish_date": "2025-07-29",
        "summary_en": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.",
        "summary_zh": "对比语言-图像预训练（CLIP）是一种流行的基础模型，支持从零样本分类、检索到多模态大语言模型（MLLM）的编码器。尽管CLIP已经在英语世界的十亿级图像-文本对上成功训练，但进一步扩展CLIP的训练以从全球网络数据中学习仍然具有挑战性：(1) 没有可用的人工筛选方法来处理非英语世界的数据点；(2) 现有多语言CLIP的英语性能比其仅英语版本差，即LLMs中常见的\"多语言诅咒\"。在这里，我们提出了MetaCLIP 2，这是第一个在全球网络规模的图像-文本对上从头开始训练CLIP的配方。为了推广我们的发现，我们进行了严格的消融实验，仅进行了必要的最小更改来解决上述挑战，并提出了一种能够从英语和非英语世界数据中实现互利共赢的配方。在零样本ImageNet分类中，MetaCLIP 2 ViT-H/14比其仅英语版本高出0.8%，比mSigLIP高出0.7%，并且在多语言基准测试上令人惊讶地创造了新的最先进水平，没有系统层面的混杂因素（例如翻译、专有架构更改），如在图像到文本检索任务上的CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600达到64.3%。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "全球多语言CLIP模型，实现英语与非英语数据的互利共赢，在多语言图像理解任务上取得最先进性能。",
        "analysis_time": "2025-08-04T19:06:11.707444"
    },
    {
        "id": "2507.22448",
        "title_en": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance",
        "title_zh": "Falcon-H1：重新定义效率和性能的混合头语言模型家族",
        "url": "https://arxiv.org/abs/2507.22448",
        "authors": "Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha",
        "publish_date": "2025-07-30",
        "summary_en": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHF Mirror Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.",
        "summary_zh": "在本报告中，我们介绍了Falcon-H1，这是一系列新型大型语言模型(LLMs)，采用混合架构设计，针对不同用例的高性能和效率进行了优化。与仅基于Transformer或Mamba架构构建的早期Falcon模型不同，Falcon-H1采用并行混合方法，结合了基于Transformer的注意力机制和状态空间模型(SSMs)，后者以其卓越的长上下文记忆和计算效率而闻名。我们系统性地重新审视了模型设计、数据策略和训练动态，挑战了该领域的传统实践。Falcon-H1以多种配置发布，包括0.5B、1.5B、1.5B-deep、3B、7B和34B参数的基础和指令微调版本。也可使用量化指令微调模型，在HF Mirror Hub上总共提供超过30个检查点。Falcon-H1模型展示了最先进的性能和卓越的参数和训练效率。旗舰模型Falcon-H1-34B匹配或超越了高达70B规模的模型，如Qwen3-32B、Qwen2.5-72B和Llama3.3-70B，同时使用更少的参数和更少的数据。较小的模型也显示出类似的趋势：Falcon-H1-1.5B-Deep可与当前领先的7B-10B模型相媲美，而Falcon-H1-0.5B的性能与2024年的典型7B模型相当。这些模型在推理、数学、多语言任务、指令遵循和科学知识方面表现出色。凭借对高达256K上下文标记和18种语言的支持，Falcon-H1适用于广泛的应用场景。所有模型均在宽松的开源许可证下发布，彰显了我们对可访问且影响深远的AI研究的承诺。",
        "github_repo": "https://github.com/tiiuae/Falcon-H1/",
        "project_page": "https://tiiuae.github.io/Falcon-H1/",
        "model_function": "混合架构大型语言模型，结合Transformer与SSM，实现高性能与高效率的平衡。",
        "analysis_time": "2025-08-04T19:06:12.915292"
    },
    {
        "id": "2507.20976",
        "title_en": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with   Weak Supervision",
        "title_zh": "利用弱监督将空中影像车辆检测器适应未见领域",
        "url": "https://arxiv.org/abs/2507.20976",
        "authors": "Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre",
        "publish_date": "2025-07-28",
        "summary_en": "Detecting vehicles in aerial imagery is a critical task with applications in\ntraffic monitoring, urban planning, and defense intelligence. Deep learning\nmethods have provided state-of-the-art (SOTA) results for this application.\nHowever, a significant challenge arises when models trained on data from one\ngeographic region fail to generalize effectively to other areas. Variability in\nfactors such as environmental conditions, urban layouts, road networks, vehicle\ntypes, and image acquisition parameters (e.g., resolution, lighting, and angle)\nleads to domain shifts that degrade model performance. This paper proposes a\nnovel method that uses generative AI to synthesize high-quality aerial images\nand their labels, improving detector training through data augmentation. Our\nkey contribution is the development of a multi-stage, multi-modal knowledge\ntransfer framework utilizing fine-tuned latent diffusion models (LDMs) to\nmitigate the distribution gap between the source and target environments.\nExtensive experiments across diverse aerial imagery domains show consistent\nperformance improvements in AP50 over supervised learning on source domain\ndata, weakly supervised adaptation methods, unsupervised domain adaptation\nmethods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than\n50%, respectively. Furthermore, we introduce two newly annotated aerial\ndatasets from New Zealand and Utah to support further research in this field.\nProject page is available at: https://humansensinglab.github.io/AGenDA",
        "summary_zh": "在空中影像中检测车辆是一项关键任务，应用于交通监控、城市规划和国防情报。深度学习方法为这一应用提供了最先进（SOTA）的结果。然而，当在一个地理区域训练的模型无法有效泛化到其他区域时，就会出现重大挑战。环境条件、城市布局、道路网络、车辆类型和图像获取参数（如分辨率、光照和角度）等因素的可变性导致了域偏移，从而降低了模型性能。本文提出了一种新颖的方法，利用生成式AI合成高质量的空中图像及其标签，通过数据增强改进检测器训练。我们的主要贡献是开发了一个多阶段、多模态知识转移框架，利用微调的潜在扩散模型（LDMs）来缩小源环境和目标环境之间的分布差距。在各种空中影像领域的广泛实验表明，与在源域数据上的监督学习、弱监督适应方法、无监督域适应方法和开放集目标检测器相比，AP50性能分别提高了4-23%、6-10%、7-40%和50%以上。此外，我们引入了来自新西兰和犹他的两个新标注的空中数据集，以支持该领域的进一步研究。项目页面可在以下网址获取：https://humansensinglab.github.io/AGenDA",
        "github_repo": "https://github.com/humansensinglab/AGenDA",
        "project_page": "https://humansensinglab.github.io/AGenDA",
        "model_function": "通过生成式AI和弱监督技术，提高空中影像车辆检测器在未见领域的泛化能力，解决域偏移问题。",
        "analysis_time": "2025-08-04T19:06:21.666022"
    },
    {
        "id": "2507.19427",
        "title_en": "Step-3 is Large yet Affordable: Model-system Co-design for   Cost-effective Decoding",
        "title_zh": "Step-3规模大却经济实惠：面向成本效益解码的模型-系统协同设计",
        "url": "https://arxiv.org/abs/2507.19427",
        "authors": "StepFun, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang",
        "publish_date": "2025-07-25",
        "summary_en": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
        "summary_zh": "大型语言模型(LLMs)在解码过程中面临硬件效率低下的问题，尤其是在长上下文推理任务中。本文介绍了Step-3，一个具有硬件感知的模型-系统协同设计优化的3210亿参数VLM(视觉语言模型)，旨在最小化解码成本。Step-3在两个关键维度上进行了创新：(1) 一种新颖的多矩阵分解注意力(MFA)机制，在保持高注意力表达能力的同时，显著减少了KV缓存大小和计算量；(2) 注意力-前馈网络解耦(AFD)，一种分布式推理系统，将注意力和前馈网络(FFN)层解耦到专门的子系统中。这种协同设计实现了前所未有的成本效率：与DeepSeek-V3和Qwen3 MoE 235B等模型相比，Step-3显著降低了理论解码成本，且在更长上下文时优势更加明显。Step-3在激活每token 380亿参数(多于DeepSeek-V3和Qwen3 MoE 235B)的同时实现了低成本，这表明硬件对齐的注意力计算强度、MoE稀疏性和AFD对成本效益至关重要。我们在DeepSeek-V3的有利场景下进行了直接比较。在Hopper GPU上的实现下，在50ms TPOT SLA(4K上下文，FP8，无MTP)条件下，每GPU解码吞吐量高达4,039 tokens/秒，高于相同设置下DeepSeek-V3的2,324，为LLM解码设立了新的帕累托前沿。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "面向长上下文推理的高效解码模型，通过创新协同设计显著降低解码成本。",
        "analysis_time": "2025-08-04T19:06:24.669645"
    },
    {
        "id": "2507.21802",
        "title_en": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
        "title_zh": "MixGRPO: 通过混合ODE-SDE释放基于流的GRPO效率",
        "url": "https://arxiv.org/abs/2507.21802",
        "authors": "Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, Zhao Zhong",
        "publish_date": "2025-07-29",
        "summary_en": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\nMixGRPO, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed MixGRPO-Flash, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\nhttps://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}.",
        "summary_zh": "尽管GRPO显著提升了图像生成中人类偏好对齐的流匹配模型，但像FlowGRPO这样的方法仍然表现出低效率，这是由于需要在马尔可夫决策过程(MDP)指定的所有去噪步骤上进行采样和优化的必要性。在本文中，我们提出了MixGRPO，一个新颖的框架，它通过整合随机微分方程(SDE)和常微分方程(ODE)，利用混合采样策略的灵活性。这简化了MDP内的优化过程，以提高效率和提升性能。具体而言，MixGRPO引入了一个滑动窗口机制，仅在窗口内使用SDE采样和GRPO引导的优化，而在窗口外应用ODE采样。这种设计将采样随机性限制在窗口内的时间步，从而减少优化开销，并允许更专注的梯度更新以加速收敛。此外，由于滑动窗口之外的时间步不参与优化，因此支持使用更高阶的求解器进行采样。因此，我们提出了一个更快的变体，称为MixGRPO-Flash，它在实现可比性能的同时进一步提高了训练效率。MixGRPO在人类偏好对齐的多个维度上表现出显著提升，在有效性和效率上都优于DanceGRPO，训练时间减少了近50%。值得注意的是，MixGRPO-Flash进一步将训练时间减少了71%。代码和模型可在https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}获取。",
        "github_repo": "https://github.com/Tencent-Hunyuan/MixGRPO",
        "project_page": "https://tulvgengenr.github.io/MixGRPO-Project-Page/",
        "model_function": "结合SDE和ODE混合采样策略，提升GRPO在流匹配模型中的训练效率，实现更快的收敛速度和更好的性能表现。",
        "analysis_time": "2025-08-04T19:06:25.015605"
    },
    {
        "id": "2507.22886",
        "title_en": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation",
        "title_zh": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation",
        "url": "https://arxiv.org/abs/2507.22886",
        "authors": "Kaining Ying, Henghui Ding, Guanquan Jie, Yu-Gang Jiang",
        "publish_date": "2025-07-30",
        "summary_en": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,098 videos and 59,458 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.",
        "summary_zh": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovis...",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "暂无",
        "analysis_time": "2025-08-04T19:06:26.005440"
    },
    {
        "id": "2507.22853",
        "title_en": "Repair-R1: Better Test Before Repair",
        "title_zh": "Repair-R1：修复前更好的测试",
        "url": "https://arxiv.org/abs/2507.22853",
        "authors": "Haichuan Hu, Xiaochen Xie, Quanjun Zhang",
        "publish_date": "2025-07-30",
        "summary_en": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://hf-mirror.com/tomhu/Qwen3-4B-RL-5000-step.",
        "summary_zh": "APR（自动程序修复）旨在自动定位程序缺陷、生成补丁并验证修复结果。现有的APR技术通常与大型语言模型（LLM）结合，利用LLM的代码相关知识来提高修复效果。当前的基于LLM的APR方法通常仅在推理阶段使用测试用例，采用先执行修复然后通过测试执行验证的迭代方法。这种传统范式忽略了两个重要方面：测试用例在训练阶段的潜在贡献，以及在修复前利用测试的可能性。为此，我们提出了Repair-R1，它将测试用例引入模型的训练阶段，并将测试生成提前到修复之前。模型需要先生成能够区分缺陷行为的判别性测试用例，然后基于这些测试进行修复。这使得模型能够更好地定位缺陷并理解缺陷的根本原因，从而提高修复效果。我们使用三种不同的骨干模型实现了Repair-R1，利用强化学习（RL）共同优化测试生成和缺陷修复。在四个广泛采用的基准测试上的实验结果证明了Repair-R1的优越性。特别是，与基础模型相比，Repair-R1将修复成功率从2.68%提高到48.29%，测试生成成功率从16.38%提高到53.28%，测试覆盖率从0.78%提高到53.96%。我们在https://github.com/Tomsawyerhu/APR-RL和https://hf-mirror.com/tomhu/Qwen3-4B-RL-5000-step发布了代码和权重。",
        "github_repo": "https://github.com/Tomsawyerhu/APR-RL",
        "project_page": "暂无",
        "model_function": "通过在修复前生成判别性测试用例，提高自动程序修复的成功率和缺陷定位准确性。",
        "analysis_time": "2025-08-04T19:06:29.966219"
    },
    {
        "id": "2507.22565",
        "title_en": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement   Learning",
        "title_zh": "通过强化学习实现大语言模型的高效差分隐私微调",
        "url": "https://arxiv.org/abs/2507.22565",
        "authors": "Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani, Gilbert Fridgen",
        "publish_date": "2025-07-30",
        "summary_en": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n(epsilon, delta)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks.",
        "summary_zh": "数据隐私与模型效用之间的紧张关系已成为在敏感语料库（包括医疗领域）上训练的大语言模型（LLMs）实际部署的关键瓶颈。差分随机梯度下降（DP-SGD）能提供正式的隐私保证，但代价高昂：梯度被强制裁剪并用噪声扰动，降低了样本效率和最终准确性。虽然已有多种方法试图缓和这种权衡，但它们都存在一个共同缺陷：其控制参数是硬编码的、全局的，并且对不断变化的优化景观一无所知。因此，实践者要么为了追求效用而过度消耗隐私预算，要么为了满足隐私约束而接受平庸的模型。我们提出了RLDP，这是首个将差分隐私优化本身作为闭环控制问题的框架，适用于现代深度强化学习（RL）。RLDP持续感知学习动态的丰富统计数据，并通过选择细粒度的每参数梯度裁剪阈值和注入的高斯噪声幅度来采取行动。在语言模型微调过程中，一个软演员-评论家（SAC）超策略在线训练；它从零开始学习如何以及何时在关键之处分配隐私预算。在GPT2-small、Llama-1B、Llama-3B和Mistral-7B上进行超过1,600次消融实验，RLDP实现了1.3-30.5%（平均5.4%）的困惑度降低和平均5.6%的下游效用提升。RLDP仅需使用13-43%（平均加速71%）的梯度更新预算即可达到每个基线的最终效用，同时遵守相同的（epsilon, delta）差分隐私合约，并且对成员推断和金丝雀提取攻击的敏感性等于或低于基线。",
        "github_repo": "https://github.com/akhadangi/RLDP",
        "project_page": "https://wandb.ai/afshin-khadangi-university-of-luxembourg/RLDP/reports/Efficient-Differentially-Private-Fine-Tuning-of-LLMs-via-Reinforcement-Learning--VmlldzoxMzc4NTEwMA?accessToken=qhs4n7sh3o93yql2wprb2vylpmer07r2bjvzft7gty5mhhplt3numljxppfd8z66",
        "model_function": "通过强化学习动态调整差分隐私参数，优化大语言模型微调过程中的隐私-效用权衡。",
        "analysis_time": "2025-08-04T19:06:34.436512"
    },
    {
        "id": "2507.13985",
        "title_en": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
        "title_zh": "DreamScene: 基于3D高斯的全端到端文本到3D场景生成",
        "url": "https://arxiv.org/abs/2507.13985",
        "authors": "Haoran Li, Yuli Tian, Kun Lan, Yong Liao, Lin Wang, Pan Hui, Peng Yuan Zhou",
        "publish_date": "2025-07-18",
        "summary_en": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://jahnsonblack.github.io/DreamScene-Full/.",
        "summary_zh": "从自然语言生成3D场景在游戏、电影和设计领域具有巨大应用前景。然而，现有方法在自动化、3D一致性和细粒度控制方面存在困难。我们提出了DreamScene，一个从文本或对话生成高质量且可编辑3D场景的全端到端框架。DreamScene首先从场景规划模块开始，其中GPT-4代理推断对象语义和空间约束以构建混合图。然后，基于图的放置算法生成结构化的无碰撞布局。基于此布局，形成模式采样(FPS)使用多时间步采样和重构优化生成对象几何形状，实现快速且真实的合成。为确保全局一致性，DreamScene采用针对室内和室外环境定制的渐进式相机采样策略。最后，该系统支持细粒度场景编辑，包括对象移动、外观变化和4D动态运动。实验证明，DreamScene在质量、一致性和灵活性方面超越了先前的方法，为开放域3D内容创作提供了实用的解决方案。代码和演示可在https://jahnsonblack.github.io/DreamScene-Full/获取。",
        "github_repo": "https://github.com/DreamScene-Project/DreamScene",
        "project_page": "https://jahnsonblack.github.io/DreamScene-Full/",
        "model_function": "基于文本或对话生成高质量且可编辑的3D场景，支持细粒度编辑和4D动态。",
        "analysis_time": "2025-08-04T19:06:37.622204"
    }
]