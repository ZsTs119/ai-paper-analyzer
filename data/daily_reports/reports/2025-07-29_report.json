[
    {
        "id": "2507.21049",
        "title_en": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for   Multi-Task Learning",
        "title_zh": "Rep-MTL: 利用表示级任务显著性增强多任务学习性能",
        "url": "https://arxiv.org/abs/2507.21049",
        "authors": "Zedong Wang, Siyuan Li, Dan Xu",
        "publish_date": "2025-07-28",
        "summary_en": "Despite the promise of Multi-Task Learning in leveraging complementary\nknowledge across tasks, existing multi-task optimization (MTO) techniques\nremain fixated on resolving conflicts via optimizer-centric loss scaling and\ngradient manipulation strategies, yet fail to deliver consistent gains. In this\npaper, we argue that the shared representation space, where task interactions\nnaturally occur, offers rich information and potential for operations\ncomplementary to existing optimizers, especially for facilitating the\ninter-task complementarity, which is rarely explored in MTO. This intuition\nleads to Rep-MTL, which exploits the representation-level task saliency to\nquantify interactions between task-specific optimization and shared\nrepresentation learning. By steering these saliencies through entropy-based\npenalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate\nnegative transfer by maintaining the effective training of individual tasks\ninstead pure conflict-solving, while explicitly promoting complementary\ninformation sharing. Experiments are conducted on four challenging MTL\nbenchmarks covering both task-shift and domain-shift scenarios. The results\nshow that Rep-MTL, even paired with the basic equal weighting policy, achieves\ncompetitive performance gains with favorable efficiency. Beyond standard\nperformance metrics, Power Law exponent analysis demonstrates Rep-MTL's\nefficacy in balancing task-specific learning and cross-task sharing. The\nproject page is available at HERE.",
        "summary_zh": "尽管多任务学习在利用任务间互补知识方面前景广阔，但现有的多任务优化(MTO)技术仍然专注于通过以优化器为中心的损失缩放和梯度操作策略来解决冲突，却未能带来持续的提升。在本文中，我们认为共享表示空间——任务交互自然发生的地方——提供了丰富的信息和进行操作的潜力，这些操作与现有优化器互补，特别是促进任务间互补性，这在MTO中很少被探索。这一直觉催生了Rep-MTL，它利用表示级任务显著性来量化特定任务优化与共享表示学习之间的交互。通过基于熵的惩罚和样本级跨任务对齐来引导这些显著性，Rep-MTL旨在通过保持各个任务的有效训练而非纯粹解决冲突来缓解负迁移，同时明确促进互补信息的共享。我们在涵盖任务偏移和域偏移场景的四个具有挑战性的多任务学习基准上进行了实验。结果表明，即使与基本的等权重策略配对，Rep-MTL也能实现具有良好效率的竞争性性能提升。除了标准性能指标外，幂律指数分析证明了Rep-MTL在平衡特定任务学习和跨任务共享方面的有效性。项目页面可在HERE获取。",
        "github_repo": "https://github.com/Jacky1128/Rep-MTL",
        "project_page": "https://jacky1128.github.io/RepMTL/",
        "model_function": "利用表示级任务显著性量化任务交互，促进互补信息共享，提升多任务学习性能。",
        "analysis_time": "2025-08-04T19:54:46.052213"
    },
    {
        "id": "2507.19849",
        "title_en": "Agentic Reinforced Policy Optimization",
        "title_zh": "智能体强化策略优化",
        "url": "https://arxiv.org/abs/2507.19849",
        "authors": "Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
        "publish_date": "2025-07-26",
        "summary_en": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO",
        "summary_zh": "具有可验证奖励的大规模强化学习(RLVR)已被证明在利用大型语言模型(LLMs)进行单回合推理任务方面是有效的。在实际推理场景中，LLMs通常可以利用外部工具来协助任务解决过程。然而，当前的RL算法无法充分平衡模型的内在长程推理能力和其在多回合工具交互中的熟练度。为了弥合这一差距，我们提出了智能体强化策略优化(ARPO)，这是一种专门用于训练多回合基于LLMs的智能体的新型智能体RL算法。通过初步实验，我们观察到LLMs在与外部工具交互后往往会表现出高度不确定的行为，表现为生成令牌的熵分布增加。受这一观察结果的启发，ARPO纳入了一种基于熵的自适应回滚机制，动态平衡全局轨迹采样和步骤级采样，从而促进工具使用后在高度不确定步骤的探索。通过整合优势归因估计，ARPO使LLMs能够内化工具使用交互中的步骤级优势差异。我们在计算推理、知识推理和深度搜索领域的13个具有挑战性的基准测试中的实验表明，ARPO优于轨迹级RL算法。值得注意的是，ARPO仅使用现有方法所需工具使用预算的一半就实现了性能提升，为将基于LLMs的智能体与实时动态环境对齐提供了可扩展的解决方案。我们的代码和数据集已在 https://github.com/dongguanting/ARPO 发布。",
        "github_repo": "https://github.com/dongguanting/ARPO",
        "project_page": "https://github.com/dongguanting/ARPO",
        "model_function": "一种基于熵的自适应回滚机制，用于训练多回合LLM智能体，平衡长程推理与多工具交互能力，优化工具使用效率。",
        "analysis_time": "2025-08-04T19:54:48.586784"
    },
    {
        "id": "2507.20984",
        "title_en": "SmallThinker: A Family of Efficient Large Language Models Natively   Trained for Local Deployment",
        "title_zh": "SmallThinker: 一类专为本地部署高效训练的大型语言模型",
        "url": "https://arxiv.org/abs/2507.20984",
        "authors": "Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen",
        "publish_date": "2025-07-28",
        "summary_en": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
        "summary_zh": "尽管前沿大型语言模型(LLMs)不断扩展能力边界，但其部署仍局限于GPU驱动的云基础设施。我们通过SmallThinker挑战这一范式，这是一类专为本地设备的独特约束而原生设计(而非适配)的LLMs：计算能力弱、内存有限和存储速度慢。与传统主要压缩为云构建的现有模型的方法不同，我们从零开始构建SmallThinker架构，使其在这些限制条件下茁壮成长。我们的创新在于一种部署感知架构，将约束转化为设计原则。首先，我们引入了一种两级稀疏结构，结合细粒度专家混合(MoE)与稀疏前馈网络，大幅减少计算需求而不牺牲模型容量。其次，为克服慢速存储的I/O瓶颈，我们设计了一个预注意力路由器，使我们协同设计的推理引擎在计算注意力时能从存储预取专家参数，有效隐藏了否则会破坏设备端推理的存储延迟。第三，为提高内存效率，我们利用NoPE-RoPE混合稀疏注意力机制大幅减少KV缓存需求。我们发布了SmallThinker-4B-A0.6B和SmallThinker-21B-A3B，它们实现了最先进的性能分数，甚至超过了更大的LLMs。值得注意的是，我们协同设计的系统基本消除了对昂贵GPU硬件的需求：采用Q4_0量化后，这两个模型在普通消费级CPU上都能超过20 tokens/s，同时仅消耗1GB和8GB内存。SmallThinker已在hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct和hf.co/PowerInfer/SmallThinker-21BA3B-Instruct上公开提供。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "专为本地设备设计的高效大型语言模型，通过创新架构解决计算资源限制，实现高性能本地部署。",
        "analysis_time": "2025-08-04T19:54:49.905729"
    },
    {
        "id": "2507.21046",
        "title_en": "A Survey of Self-Evolving Agents: On Path to Artificial Super   Intelligence",
        "title_zh": "自进化智能体综述：迈向人工智能超级智能之路",
        "url": "https://arxiv.org/abs/2507.21046",
        "authors": "Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang",
        "publish_date": "2025-07-28",
        "summary_en": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
        "summary_zh": "大型语言模型(LLMs)已展现出强大的能力，但本质上仍然是静态的，无法将其内部参数适应到新任务、不断发展的知识领域或动态交互上下文中。随着LLMs越来越多地部署在开放式、交互式环境中，这种静态特性已成为关键瓶颈，需要能够实时自适应推理、行动和演化的智能体。这一范式转变——从扩展静态模型到开发自进化智能体——激发了人们对能够从数据、交互和经验中实现持续学习和适应的架构与方法的日益浓厚兴趣。本综述首次对自进化智能体进行了系统全面的回顾，围绕三个基础维度组织——演化什么、何时演化以及如何演化。我们检查了跨智能体组件的演化机制（例如，模型、内存、工具、架构），按阶段对适应方法进行分类（例如，测试内、测试间），并分析引导演化适应的算法和架构设计（例如，标量奖励、文本反馈、单智能体和多智能体系统）。此外，我们分析了专为自进化智能体设计的评估指标和基准，突显了在编码、教育和医疗等领域的应用，并确定了在安全性、可扩展性和协同演化动态方面的关键挑战和研究方向。通过为理解和设计自进化智能体提供结构化框架，本综述为推进研究和实际部署中的自适应智能体系统建立了路线图，最终为人工智能超级智能(ASI)的实现铺平道路，在ASI中，智能体自主演化，能够在广泛任务中达到或超越人类水平的智能。",
        "github_repo": "https://github.com/CharlesQ9/Self-Evolving-Agents",
        "project_page": "暂无",
        "model_function": "综述自进化智能体架构与演化机制，为迈向人工智能超级智能提供研究框架和方向。",
        "analysis_time": "2025-08-04T19:54:50.009724"
    },
    {
        "id": "2507.20939",
        "title_en": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World   Shorts",
        "title_zh": "ARC-Hunyuan-Video-7B: 真实世界短视频的结构化视频理解",
        "url": "https://arxiv.org/abs/2507.20939",
        "authors": "Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, Jinwen Luo, Weibo Gu, Zexuan Li, Xiaojing Zhang, Yangyu Tao, Han Hu, Di Wang, Ying Shan",
        "publish_date": "2025-07-28",
        "summary_en": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
        "summary_zh": "真实世界用户生成的短视频，特别是在微信频道和抖音等平台上传播的短视频，主导着移动互联网。然而，当前的大型多模态模型缺乏必要的时序结构化、详细和深入的视频理解能力，而这些能力是有效视频搜索和推荐以及新兴视频应用的基石。理解真实世界的短视频实际上具有挑战性，因为它们包含复杂的视觉元素，视听信息密度高，且节奏快速，专注于情感表达和观点传递。这需要先进的推理能力来有效整合视觉、音频和文本等多模态信息。在这项工作中，我们介绍了ARC-Hunyuan-Video，一个多模态模型，它从头到尾处理原始视频输入中的视觉、音频和文本信号，以实现结构化理解。该模型能够进行多粒度带时间戳的视频字幕生成和摘要、开放式视频问答、时序视频定位和视频推理。利用来自自动化标注流程的高质量数据，我们紧凑的70亿参数模型通过全面的训练流程进行训练：预训练、指令微调、冷启动、强化学习(RL)后训练和最终指令微调。在我们引入的基准ShortVid-Bench上进行定量评估和定性比较，展示了其在真实世界视频理解方面的强大性能，并支持零样本或少样本微调，适用于各种下游应用。我们模型的实际生产部署已在用户参与度和满意度方面带来了切实可衡量的改进，这一成功得益于其卓越的效率，压力测试表明在H20 GPU上处理一分钟视频的推理时间仅为10秒。",
        "github_repo": "https://github.com/TencentARC/ARC-Hunyuan-Video-7B",
        "project_page": "https://tencentarc.github.io/posts/arc-video-announcement/",
        "model_function": "多模态视频理解，支持字幕生成、问答、定位和推理，应用于真实世界短视频分析",
        "analysis_time": "2025-08-04T19:54:52.757942"
    },
    {
        "id": "2507.20673",
        "title_en": "Geometric-Mean Policy Optimization",
        "title_zh": "几何平均策略优化",
        "url": "https://arxiv.org/abs/2507.20673",
        "authors": "Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei",
        "publish_date": "2025-07-28",
        "summary_en": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have\nenhanced the reasoning capabilities of large language models by optimizing the\narithmetic mean of token-level rewards. However, GRPO suffers from unstable\npolicy updates when processing tokens with outlier importance-weighted rewards,\nwhich manifests as extreme importance sampling ratios during training, i.e.,\nthe ratio between the sampling probabilities assigned to a token by the current\nand old policies. In this work, we propose Geometric-Mean Policy Optimization\n(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic\nmean, GMPO maximizes the geometric mean of token-level rewards, which is\ninherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical\nand experimental analysis to justify the design and stability benefits of GMPO.\nBeyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on\nmultiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,\nincluding AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is\navailable at https://github.com/callsys/GMPO.",
        "summary_zh": "最近的进展，如组相对策略优化（GRPO），通过优化标记级奖励的算术平均值，增强了大型语言模型的推理能力。然而，GRPO在处理具有异常值重要性加权奖励的标记时，会遇到不稳定的策略更新问题，这表现为训练期间极端的重要性采样比率，即当前策略和旧策略分配给标记的采样概率之间的比率。在这项工作中，我们提出了几何平均策略优化（GMPO），这是GRPO的一个稳定变体。GMPO不是优化算术平均值，而是最大化标记级奖励的几何平均值，这本质上对异常值不那么敏感，并保持更稳定的重要性采样比率范围。此外，我们提供了全面的理论和实验分析，以证明GMPO的设计和稳定性优势。除了提高稳定性外，GMPO-7B在多个数学基准测试上的平均表现比GRPO高出4.1%，在多模态推理基准测试上高出1.4%，包括AIME24、AMC、MATH500、OlympiadBench、Minerva和Geometry3K。代码可在https://github.com/callsys/GMPO获取。",
        "github_repo": "https://github.com/callsys/GMPO",
        "project_page": "暂无",
        "model_function": "基于几何平均的奖励优化，提升语言模型推理能力与稳定性",
        "analysis_time": "2025-08-04T19:54:59.420169"
    },
    {
        "id": "2507.21045",
        "title_en": "Reconstructing 4D Spatial Intelligence: A Survey",
        "title_zh": "重建四维空间智能：综述",
        "url": "https://arxiv.org/abs/2507.21045",
        "authors": "Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu",
        "publish_date": "2025-07-28",
        "summary_en": "Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
        "summary_zh": "从视觉观测中重建四维空间智能长期以来一直是计算机视觉中一个核心且具有挑战性的任务，具有广泛的实际应用。这些应用范围从娱乐领域如电影（通常专注于重建基本视觉元素）到具身人工智能（强调交互建模和物理真实性）。受3D表示和深度学习架构快速进步的推动，该领域发展迅速，超越了先前综述的范围。此外，现有综述很少对四维场景重建的层次结构进行全面分析。为填补这一空白，我们提出了一种新视角，将现有方法组织为五个递进级别的四维空间智能：（1）第1级——重建低级3D属性（例如深度、姿态和点云图）；（2）第2级——重建3D场景组件（例如物体、人类、结构）；（3）第3级——重建4D动态场景；（4）第4级——建模场景组件之间的交互；以及（5）第5级——融入物理定律和约束。我们在综述的最后讨论了每个级别的关键挑战，并指出了推进到更丰富四维空间智能水平的有前途的方向。为了跟踪持续的发展，我们维护了一个最新的项目页面：https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence。",
        "github_repo": "https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence",
        "project_page": "暂无",
        "model_function": "综述四维空间智能重建的五级层次结构，分析各层级挑战与未来方向。",
        "analysis_time": "2025-08-04T19:54:59.679147"
    },
    {
        "id": "2507.21033",
        "title_en": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
        "title_zh": "GPT-IMAGE-EDIT-1.5M：一个百万规模的GPT生成图像数据集",
        "url": "https://arxiv.org/abs/2507.21033",
        "authors": "Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, Cihang Xie",
        "publish_date": "2025-07-28",
        "summary_en": "Recent advancements in large multimodal models like GPT-4o have set a new\nstandard for high-fidelity, instruction-guided image editing. However, the\nproprietary nature of these models and their training data creates a\nsignificant barrier for open-source research. To bridge this gap, we introduce\nGPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus\ncontaining more than 1.5 million high-quality triplets (instruction, source\nimage, edited image). We systematically construct this dataset by leveraging\nthe versatile capabilities of GPT-4o to unify and refine three popular\nimage-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our\nmethodology involves 1) regenerating output images to enhance visual quality\nand instruction alignment, and 2) selectively rewriting prompts to improve\nsemantic clarity. To validate the efficacy of our dataset, we fine-tune\nadvanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are\nexciting, e.g., the fine-tuned FluxKontext achieves highly competitive\nperformance across a comprehensive suite of benchmarks, including 7.24 on\nGEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger\ninstruction following and higher perceptual quality while maintaining identity.\nThese scores markedly exceed all previously published open-source methods and\nsubstantially narrow the gap to leading proprietary models. We hope the full\nrelease of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in\ninstruction-guided image editing.",
        "summary_zh": "像GPT-4o这样的大型多模态模型的最新进展为高保真、指令引导的图像编辑树立了新标准。然而，这些模型及其训练数据的专有性质为开源研究造成了重大障碍。为了弥合这一差距，我们推出了GPT-IMAGE-EDIT-1.5M，这是一个公开可用的、大规模图像编辑语料库，包含超过150万组高质量三元组（指令、源图像、编辑后图像）。我们利用GPT-4o的多功能能力系统地构建了这个数据集，以统一和优化三个流行的图像编辑数据集：OmniEdit、HQ-Edit和UltraEdit。具体来说，我们的方法包括：1) 重新生成输出图像以提高视觉质量和指令对齐度，以及2) 选择性重写提示以提高语义清晰度。为了验证我们数据集的有效性，我们在GPT-IMAGE-EDIT-1.5M上对先进的开源模型进行了微调。实验结果令人振奋，例如，微调后的FluxKontext在全面的基准测试中实现了极具竞争力的性能，包括在GEdit-EN上达到7.24分，在ImgEdit-Full上达到3.80分，在Complex-Edit上达到8.78分，显示出更强的指令遵循能力和更高的感知质量，同时保持身份一致性。这些分数明显超过了所有先前发布的开源方法，并显著缩小了与领先专有模型的差距。我们希望GPT-IMAGE-EDIT-1.5M的完整发布能够促进指令引导图像编辑领域的进一步开放研究。",
        "github_repo": "https://github.com/wyhlovecpp/GPT-Image-Edit",
        "project_page": "https://ucsc-vlaa.github.io/GPT-Image-Edit/",
        "model_function": "提供大规模高质量图像编辑数据集，用于训练开源图像编辑模型，缩小与专有模型的性能差距。",
        "analysis_time": "2025-08-04T19:55:02.217827"
    },
    {
        "id": "2507.20025",
        "title_en": "Region-based Cluster Discrimination for Visual Representation Learning",
        "title_zh": "基于区域聚类的视觉表征学习",
        "url": "https://arxiv.org/abs/2507.20025",
        "authors": "Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, Ismail Elezi, Jiankang Deng",
        "publish_date": "2025-07-26",
        "summary_en": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
        "summary_zh": "学习视觉表征是广泛下游任务的基础。尽管最近的视觉-语言对比模型，如CLIP和SigLIP，通过大规模视觉-语言对齐取得了令人印象深刻的零样本性能，但它们对全局表征的依赖限制了它们在密集预测任务（如定位、OCR和分割）中的有效性。为解决这一差距，我们引入了区域感知聚类判别(RICE)，这是一种增强区域级视觉和OCR能力的新方法。我们首先构建了一个十亿规模的候选区域数据集，并提出了一种区域Transformer层来提取丰富的区域语义。我们进一步设计了一个统一的区域聚类判别损失，该损失在单一分类框架内共同支持对象和OCR学习，使大规模数据上的高效可扩展分布式训练成为可能。大量实验表明，RICE在包括分割、密集检测和多模态大语言模型(MLLMs)视觉感知在内的任务上始终优于先前的方法。预训练模型已在https://github.com/deepglint/MVT发布。",
        "github_repo": "https://github.com/deepglint/MVT",
        "project_page": "暂无",
        "model_function": "基于区域聚类的视觉表征增强方法，提升密集预测任务性能",
        "analysis_time": "2025-08-04T19:55:02.233696"
    },
    {
        "id": "2507.20187",
        "title_en": "Diversity-Enhanced Reasoning for Subjective Questions",
        "title_zh": "面向主观问题的多样性增强推理",
        "url": "https://arxiv.org/abs/2507.20187",
        "authors": "Yumeng Wang, Zhiyuan Fan, Jiayu Liu, Yi R. Fung",
        "publish_date": "2025-07-27",
        "summary_en": "Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities\nhave shown strong performance on objective tasks, such as math reasoning and\ncoding. However, their effectiveness on subjective questions that may have\ndifferent responses from different perspectives is still limited by a tendency\ntowards homogeneous reasoning, introduced by the reliance on a single ground\ntruth in supervised fine-tuning and verifiable reward in reinforcement\nlearning. Motivated by the finding that increasing role perspectives\nconsistently improves performance, we propose MultiRole-R1, a\ndiversity-enhanced framework with multiple role perspectives, to improve the\naccuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an\nunsupervised data construction pipeline that generates reasoning chains that\nincorporate diverse role perspectives. We further employ reinforcement learning\nvia Group Relative Policy Optimization (GRPO) with reward shaping, by taking\ndiversity as a reward signal in addition to the verifiable reward. With\nspecially designed reward functions, we successfully promote perspective\ndiversity and lexical diversity, uncovering a positive relation between\nreasoning diversity and accuracy. Our experiment on six benchmarks demonstrates\nMultiRole-R1's effectiveness and generalizability in enhancing both subjective\nand objective reasoning, showcasing the potential of diversity-enhanced\ntraining in LRMs.",
        "summary_zh": "具有长链思维(CoT)能力的大型推理模型(LRM)在数学推理和编程等客观任务上已展现出强大的性能。然而，对于那些可能因不同视角而产生不同回答的主观问题，它们的有效性仍然受到同质化推理倾向的限制，而这种倾向源于监督微调中对单一真实答案的依赖以及强化学习中可验证奖励的使用。受增加角色视角能够持续提升性能这一发现的启发，我们提出了MultiRole-R1，这是一个具有多种角色视角的多样性增强框架，旨在提高主观推理任务中的准确性和多样性。MultiRole-R1具有一个无监督数据构建管道，能够生成融入多种角色视角的推理链。我们进一步通过采用带有奖励塑造的组相对策略优化(GRPO)来应用强化学习，将多样性作为除可验证奖励之外的奖励信号。通过专门设计的奖励函数，我们成功促进了视角多样性和词汇多样性，揭示了推理多样性与准确性之间的正相关关系。我们在六个基准测试上的实验证明了MultiRole-R1在增强主观和客观推理方面的有效性和通用性，展示了多样性增强训练在大型推理模型中的潜力。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "通过多角色视角增强推理框架，提高主观推理任务的准确性和多样性",
        "analysis_time": "2025-08-04T19:55:04.820576"
    },
    {
        "id": "2507.19766",
        "title_en": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing   Large Language Models' Reasoning Abilities",
        "title_zh": "UloRL：一种推进大型语言模型推理能力的超长输出强化学习方法",
        "url": "https://arxiv.org/abs/2507.19766",
        "authors": "Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, Yang Li",
        "publish_date": "2025-07-26",
        "summary_en": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.",
        "summary_zh": "近期大型语言模型（LLMs）的进展凸显了通过可验证奖励的强化学习（RLVR）增强推理能力的潜力，该方法通过扩展输出序列实现。然而，当处理超长输出时，传统的强化学习框架面临效率低下的问题，这主要是由于长尾序列分布和训练过程中的熵崩溃。为应对这些挑战，我们提出了一种超长输出强化学习（UloRL）方法，用于提升大型语言模型的推理能力。具体而言，我们将超长输出解码划分为短片段，通过减轻长尾样本引起的延迟，实现高效训练。此外，我们引入了对已掌握的正向标记（MPTs）的动态掩码，以防止熵崩溃。实验结果证明了我们方法的有效性。在Qwen3-30B-A3B模型上，采用分段展开的强化学习实现了2.06倍的速度提升，而使用128k-token输出的强化学习训练则将模型在AIME2025上的性能从70.9%提高到85.1%，在BeyondAIME上从50.7%提高到61.9%，甚至显著超越了Qwen3-235B-A22B的性能。这些研究结果强调了我们的方法在通过超长序列生成推进LLMs推理能力方面的潜力。我们将发布我们的代码和模型，供社区进一步使用。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "通过分段解码和动态掩码技术，解决长尾序列分布和熵崩溃问题，提升大型语言模型的推理能力。",
        "analysis_time": "2025-08-04T19:55:09.418105"
    },
    {
        "id": "2507.17189",
        "title_en": "Met^2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for   Complex Meteorological Systems",
        "title_zh": "Met^2Net：一种用于复杂气象系统的解耦两阶段时空预测模型",
        "url": "https://arxiv.org/abs/2507.17189",
        "authors": "Shaohan Li, Hao Yang, Min Chen, Xiaolin Qin",
        "publish_date": "2025-07-23",
        "summary_en": "The increasing frequency of extreme weather events due to global climate\nchange urges accurate weather prediction. Recently, great advances have been\nmade by the end-to-end methods, thanks to deep learning techniques,\nbut they face limitations of representation inconsistency in\nmultivariable integration and struggle to effectively capture the dependency\nbetween variables, which is required in complex weather systems. Treating\ndifferent variables as distinct modalities and applying a two-stage\ntraining approach from multimodal models can partially alleviate this issue,\nbut due to the inconformity in training tasks between the two stages, the\nresults are often suboptimal. To address these challenges, we propose an\nimplicit two-stage training method, configuring separate encoders and decoders\nfor each variable. In detailed, in the first stage, the Translator is frozen\nwhile the Encoders and Decoders learn a shared latent space, in the second\nstage, the Encoders and Decoders are frozen, and the Translator captures\ninter-variable interactions for prediction. Besides, by introducing a\nself-attention mechanism for multivariable fusion in the latent space, the\nperformance achieves further improvements. Empirically, extensive experiments\nshow the state-of-the-art performance of our method. Specifically, it reduces\nthe MSE for near-surface air temperature and relative humidity predictions by\n28.82\\% and 23.39\\%, respectively. The source code is available at\nhttps://github.com/ShremG/Met2Net.",
        "summary_zh": "由于全球气候变化导致极端天气事件日益频繁，准确的天气预报变得至关重要。近年来，得益于深度学习技术，端到端方法取得了重大进展，但在多变量集成中存在表示不一致性的限制，并且难以有效捕捉复杂气象系统中所需的变量间依赖关系。将不同变量视为不同模态并应用来自多模态模型的两阶段训练方法可以部分缓解这一问题，但由于两个阶段训练任务的不一致性，结果往往不尽如人意。为解决这些挑战，我们提出了一种隐式两阶段训练方法，为每个变量配置独立的编码器和解码器。具体而言，在第一阶段，翻译器保持冻结状态，而编码器和解码器学习共享的潜在空间；在第二阶段，编码器和解码器保持冻结，翻译器捕获变量间交互以进行预测。此外，通过在潜在空间中引入多变量融合的自注意力机制，性能得到了进一步提升。实验表明，大量实验证明了我们方法的最新性能。具体而言，它将近地表空气温度和相对湿度预测的均方误差分别降低了28.82%和23.39%。源代码可在 https://github.com/ShremG/Met2Net 获取。",
        "github_repo": "https://github.com/ShremG/Met2Net",
        "project_page": "暂无",
        "model_function": "复杂气象系统的时空预测，通过解耦两阶段训练方法捕捉变量间依赖关系，提高天气预报准确性。",
        "analysis_time": "2025-08-04T19:55:09.978721"
    },
    {
        "id": "2507.20880",
        "title_en": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability   and Aesthetic Alignment",
        "title_zh": "JAM：一款具有细粒度可控性和美学对齐功能的小型基于流的歌曲生成器",
        "url": "https://arxiv.org/abs/2507.20880",
        "authors": "Renhang Liu, Chia-Yu Hung, Navonil Majumder, Taylor Gautreaux, Amir Ali Bagherzadeh, Chuan Li, Dorien Herremans, Soujanya Poria",
        "publish_date": "2025-07-28",
        "summary_en": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
        "summary_zh": "扩散模型和流匹配模型最近彻底改变了自动文本到音频生成。这些模型越来越能够生成高质量、忠实的音频输出，捕捉语音和声学事件。然而，在主要涉及音乐和歌曲的创意音频生成方面仍有很大的改进空间。最近的开放歌词到歌曲模型，如DiffRhythm、ACE-Step和LeVo，已经为休闲用途的自动歌曲生成设定了可接受的标准。然而，这些模型缺乏音乐人在工作流程中通常需要的细粒度词级可控性。据我们所知，我们基于流匹配的JAM是首次尝试在歌曲生成中赋予词级时间和持续时间控制，从而实现细粒度的声乐控制。为了提高生成歌曲的质量，使其更好地符合人类偏好，我们通过直接偏好优化实现了美学对齐，该方法使用合成数据集迭代地改进模型，消除了手动数据标注的需要。此外，我们旨在通过我们的公共评估数据集JAME来标准化此类歌词到歌曲模型的评估。我们表明，JAM在音乐特定属性方面优于现有模型。",
        "github_repo": "https://github.com/declare-lab/jamify",
        "project_page": "https://declare-lab.github.io/jamify",
        "model_function": "基于流匹配的歌曲生成器，提供词级时间控制和美学对齐功能，生成高质量音乐。",
        "analysis_time": "2025-08-04T19:55:12.355897"
    },
    {
        "id": "2507.19804",
        "title_en": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
        "title_zh": "ForCenNet: 前景中心网络用于文档图像校正",
        "url": "https://arxiv.org/abs/2507.19804",
        "authors": "Peng Cai, Qiang Li, Kaicheng Yang, Dong Guo, Jia Li, Nan Zhou, Xiang An, Ninghua Yang, Jiankang Deng",
        "publish_date": "2025-07-26",
        "summary_en": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.",
        "summary_zh": "文档图像校正旨在消除拍摄文档中的几何变形，以便于文本识别。然而，现有方法常常忽略前景元素的重要性，这些元素为文档图像校正提供了必要的几何参考和布局信息。在本文中，我们引入了前景中心网络(ForCenNet)来消除文档图像中的几何失真。具体而言，我们首先提出了一种前景中心标签生成方法，该方法从未失真图像中提取详细的前景元素。然后，我们引入了一种前景中心掩码机制，以增强可读区域和背景区域之间的区分度。此外，我们设计了一种曲率一致性损失，利用详细的前景标签来帮助模型理解失真的几何分布。大量实验表明，ForCenNet在DocUNet、DIR300、WarpDoc和DocReal四个真实世界基准测试上取得了新的最先进性能。定量分析表明，所提出的方法能够有效校正布局元素，如文本行和表格边框。进一步比较的资源可在https://github.com/caipeng328/ForCenNet获取。",
        "github_repo": "https://github.com/caipeng328/ForCenNet",
        "project_page": "暂无",
        "model_function": "ForCenNet是一个专注于前景元素的文档图像校正网络，通过提取前景参考信息消除文档图像中的几何变形，提升文本识别效果。",
        "analysis_time": "2025-08-04T19:55:13.845872"
    },
    {
        "id": "2507.19058",
        "title_en": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with   Concept Relation Alignment",
        "title_zh": "场景绘制者：通过概念关系对齐实现语义一致的持续3D场景生成",
        "url": "https://arxiv.org/abs/2507.19058",
        "authors": "Chong Xia, Shengjun Zhang, Fangfu Liu, Chang Liu, Khodchaphun Hirunyaratsameewong, Yueqi Duan",
        "publish_date": "2025-07-25",
        "summary_en": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view\nsequences, which is applicable for long-term video synthesis and 3D scene\nreconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and\nrely on outpainting for successive view expansion. However, the generated view\nsequences suffer from semantic drift issue derived from the accumulated\ndeviation of the outpainting module. To tackle this challenge, we propose\nScenePainter, a new framework for semantically consistent 3D scene generation,\nwhich aligns the outpainter's scene-specific prior with the comprehension of\nthe current scene. To be specific, we introduce a hierarchical graph structure\ndubbed SceneConceptGraph to construct relations among multi-level scene\nconcepts, which directs the outpainter for consistent novel views and can be\ndynamically refined to enhance diversity. Extensive experiments demonstrate\nthat our framework overcomes the semantic drift issue and generates more\nconsistent and immersive 3D view sequences. Project Page:\nhttps://xiac20.github.io/ScenePainter/.",
        "summary_zh": "持续3D场景生成旨在产生长距离且连贯的3D视图序列，适用于长期视频合成和3D场景重建。现有方法遵循\"导航与想象\"的模式，并依靠外扩(outpainting)技术进行连续视图扩展。然而，生成的视图序列因外扩模块的累积偏差而遭受语义漂移问题。为应对这一挑战，我们提出了ScenePainter，这是一个用于语义一致的3D场景生成的新框架，它将外扩器的场景特定先验与对当前场景的理解进行对齐。具体而言，我们引入了一个名为场景概念图(SceneConceptGraph)的分层图结构，用于构建多级场景概念之间的关系，这指导外扩器生成一致的新视图，并可动态优化以提高多样性。大量实验证明，我们的框架克服了语义漂移问题，并生成了更加一致和沉浸式的3D视图序列。项目页面：https://xiac20.github.io/ScenePainter/。GitHub仓库：https://github.com/xiac20/ScenePainter 项目页面：https://xiac20.github.io/ScenePainter/",
        "github_repo": "https://github.com/xiac20/ScenePainter",
        "project_page": "https://xiac20.github.io/ScenePainter/",
        "model_function": "解决3D场景生成中的语义漂移问题，生成语义一致且沉浸式的持续3D视图序列",
        "analysis_time": "2025-08-04T19:55:14.056008"
    },
    {
        "id": "2507.21848",
        "title_en": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for   Advantage Diversity",
        "title_zh": "EDGE-GRPO：用于优势多样性的熵驱动GRPO与引导错误校正",
        "url": "https://arxiv.org/abs/2507.21848",
        "authors": "Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang",
        "publish_date": "2025-07-29",
        "summary_en": "Large Language Models (LLMs) have made remarkable progress in enhancing\nstep-by-step reasoning through reinforcement learning. However, the Group\nRelative Policy Optimization (GRPO) algorithm, which relies on sparse reward\nrules, often encounters the issue of identical rewards within groups, leading\nto the advantage collapse problem. Existing works typically address this\nchallenge from two perspectives: enforcing model reflection to enhance response\ndiversity, and introducing internal feedback to augment the training signal\n(advantage). In this work, we begin by analyzing the limitations of model\nreflection and investigating the policy entropy of responses at the\nfine-grained sample level. Based on our experimental findings, we propose the\nEDGE-GRPO algorithm, which adopts Entropy-Driven Advantage\nand Guided Error Correction to effectively mitigate the\nproblem of advantage collapse. Extensive experiments on several main reasoning\nbenchmarks demonstrate the effectiveness and superiority of our approach. It is\navailable at https://github.com/ZhangXJ199/EDGE-GRPO.",
        "summary_zh": "大型语言模型（LLMs）通过强化学习在增强逐步推理方面取得了显著进展。然而，依赖于稀疏奖励规则的组相对策略优化（GRPO）算法经常遇到组内奖励相同的问题，导致优势崩溃问题。现有工作通常从两个角度解决这一挑战：强制模型反思以提高响应多样性，以及引入内部反馈来增强训练信号（优势）。在这项工作中，我们首先分析了模型反思的局限性，并研究了细粒度样本级别上响应的策略熵。基于我们的实验发现，我们提出了EDGE-GRPO算法，该算法采用熵驱动优势和引导错误校正，有效缓解了优势崩溃问题。在几个主要推理基准上的大量实验证明了我们方法的有效性和优越性。它可在 https://github.com/ZhangXJ199/EDGE-GRPO 获取。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "解决GRPO算法优势崩溃问题，通过熵驱动优势和引导错误校正提升模型推理能力和响应多样性。",
        "analysis_time": "2025-08-04T19:55:17.733920"
    },
    {
        "id": "2507.20900",
        "title_en": "Music Arena: Live Evaluation for Text-to-Music",
        "title_zh": "Music Arena：文本到音乐的实时评估",
        "url": "https://arxiv.org/abs/2507.20900",
        "authors": "Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue",
        "publish_date": "2025-07-28",
        "summary_en": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
        "summary_zh": "我们提出了Music Arena，一个用于可扩展人类偏好评估文本到音乐(TTM)模型的开放平台。通过听力研究征求人类偏好是TTM评估的金标准，但这些研究成本高昂且难以比较，因为不同系统的研究协议可能不同。此外，人类偏好可能帮助研究人员调整其TTM系统或改进自动评估指标，但目前不存在开放且可再生的偏好来源。我们旨在通过为TTM提供*实时*评估来填补这些空白。在Music Arena中，现实世界的用户输入他们选择的文本提示，并比较来自两个TTM系统的输出，他们的偏好被用来编制排行榜。虽然Music Arena遵循了其他AI领域的最新评估趋势，但我们还设计了针对音乐的关键功能：基于LLM的路由系统来导航TTM系统的异构类型签名，以及收集包括听力和自然语言反馈在内的*详细*偏好。我们还提出了滚动数据发布政策与用户隐私保证，提供可再生的偏好数据来源并增加平台透明度。通过其标准化的评估协议、透明的数据访问政策和音乐特定功能，Music Arena不仅解决了TTM生态系统中的关键挑战，还展示了如何将实时评估深思熟虑地适应特定AI领域的独特特征。Music Arena可在以下网址访问：https://music-arena.org",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "文本到音乐模型的实时人类偏好评估平台，通过标准化协议解决TTM评估挑战",
        "analysis_time": "2025-08-04T19:55:19.332429"
    },
    {
        "id": "2507.16806",
        "title_en": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
        "title_zh": "超越二元奖励：训练语言模型对其不确定性进行推理",
        "url": "https://arxiv.org/abs/2507.16806",
        "authors": "Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas",
        "publish_date": "2025-07-22",
        "summary_en": "When language models (LMs) are trained via reinforcement learning (RL) to\ngenerate natural language \"reasoning chains\", their performance improves on a\nvariety of difficult question answering tasks. Today, almost all successful\napplications of RL for reasoning use binary reward functions that evaluate the\ncorrectness of LM outputs. Because such reward functions do not penalize\nguessing or low-confidence outputs, they often have the unintended side-effect\nof degrading calibration and increasing the rate at which LMs generate\nincorrect responses (or \"hallucinate\") in other problem domains. This paper\ndescribes RLCR (Reinforcement Learning with Calibration Rewards), an approach\nto training reasoning models that jointly improves accuracy and calibrated\nconfidence estimation. During RLCR, LMs generate both predictions and numerical\nconfidence estimates after reasoning. They are trained to optimize a reward\nfunction that augments a binary correctness score with a Brier score -- a\nscoring rule for confidence estimates that incentivizes calibrated prediction.\nWe first prove that this reward function (or any analogous reward function that\nuses a bounded, proper scoring rule) yields models whose predictions are both\naccurate and well-calibrated. We next show that across diverse datasets, RLCR\nsubstantially improves calibration with no loss in accuracy, on both in-domain\nand out-of-domain evaluations -- outperforming both ordinary RL training and\nclassifiers trained to assign post-hoc confidence scores. While ordinary RL\nhurts calibration, RLCR improves it. Finally, we demonstrate that verbalized\nconfidence can be leveraged at test time to improve accuracy and calibration\nvia confidence-weighted scaling methods. Our results show that explicitly\noptimizing for calibration can produce more generally reliable reasoning\nmodels.",
        "summary_zh": "当语言模型(LMs)通过强化学习(RL)训练以生成自然语言\"推理链\"时，它们在各种困难的问答任务上表现更好。如今，几乎所有用于推理的成功RL应用都使用二元奖励函数来评估LM输出的正确性。由于这类奖励函数不会惩罚猜测或低置信度的输出，它们常常会产生意外的副作用，降低校准能力，并增加LM在其他问题领域中生成不正确响应(或\"产生幻觉\")的速率。本文描述了RLCR(带校准奖励的强化学习)，这是一种训练推理模型的方法，可以同时提高准确性和校准置信度估计。在RLCR过程中，LM在推理后生成预测和数值置信度估计。它们被训练以优化奖励函数，该函数将二元正确性分数与Brier分数(一种置信度估计的评分规则)相结合，该规则激励校准预测。我们首先证明，这种奖励函数(或任何使用有界、适当评分规则的类似奖励函数)产生的模型，其预测既准确又具有良好的校准性。接下来，我们展示在各种数据集上，RLCR在领域内和领域外评估中显著提高了校准能力，同时没有损失准确性，优于普通RL训练和分配事后置信度分数的分类器。虽然普通RL会损害校准能力，但RLCR却能改善它。最后，我们证明在测试时可以利用语言化的置信度，通过置信度加权缩放方法来提高准确性和校准能力。我们的结果表明，明确优化校准可以产生更普遍可靠的推理模型。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "训练语言模型生成带有校准置信度估计的推理链，提高预测准确性和可靠性",
        "analysis_time": "2025-08-04T19:55:20.006729"
    },
    {
        "id": "2507.20527",
        "title_en": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful   Mathematics Questions and Answers",
        "title_zh": "SAND-Math：利用大语言模型生成新颖、困难且有用的数学问题和答案",
        "url": "https://arxiv.org/abs/2507.20527",
        "authors": "Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, Emad Barsoum",
        "publish_date": "2025-07-28",
        "summary_en": "The demand for Large Language Models (LLMs) capable of sophisticated\nmathematical reasoning is growing across industries. However, the development\nof performant mathematical LLMs is critically bottlenecked by the scarcity of\ndifficult, novel training data. We introduce SAND-Math (Synthetic\nAugmented Novel and Difficult Mathematics problems and solutions), a pipeline\nthat addresses this by first generating high-quality problems from scratch and\nthen systematically elevating their complexity via a new Difficulty\nHiking step. We demonstrate the effectiveness of our approach through two key\nfindings. First, augmenting a strong baseline with SAND-Math data significantly\nboosts performance, outperforming the next-best synthetic dataset by\nuparrow 17.85 absolute points on the AIME25 benchmark. Second, in a\ndedicated ablation study, we show our Difficulty Hiking process is highly\neffective: by increasing average problem difficulty from 5.02 to 5.98, this\nstep lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation\npipeline, final dataset, and a fine-tuned model form a practical and scalable\ntoolkit for building more capable and efficient mathematical reasoning LLMs.\nSAND-Math dataset is released here:\nhttps://hf-mirror.com/datasets/amd/SAND-MATH{https://hf-mirror.com/datasets/amd/SAND-MATH}",
        "summary_zh": "各行各业对能够进行复杂数学推理的大型语言模型(LLMs)的需求正在增长。然而，高性能数学LLM的发展受到困难、新颖训练数据稀缺的关键瓶颈限制。我们介绍了SAND-Math（合成增强新颖和困难的数学问题和解决方案），这是一个通过首先从头开始生成高质量问题，然后通过新的难度提升步骤系统地提高其复杂度来解决这一问题的流程。我们通过两个关键发现证明了我们方法的有效性。首先，用SAND-Math数据增强一个强大的基线模型显著提高了性能，在AIME25基准测试上比次优合成数据集高出17.85个绝对点。其次，在专门的消融研究中，我们展示了我们的难度提升过程非常有效：通过将平均问题难度从5.02提高到5.98，这一步骤将AIME25性能从46.38%提升到49.23%。完整的生成流程、最终数据集和微调模型构成了构建更强大、更高效的数学推理LLM的实用且可扩展的工具包。SAND-Math数据集在此发布：https://hf-mirror.com/datasets/amd/SAND-MATH",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "生成高质量数学问题并提升难度，解决数学LLM训练数据稀缺问题，提升数学推理能力。",
        "analysis_time": "2025-08-04T19:55:20.690235"
    },
    {
        "id": "2507.19399",
        "title_en": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
        "title_zh": "在CIRCLE中运行？一个针对LLM代码解释器安全性的简单基准测试",
        "url": "https://arxiv.org/abs/2507.19399",
        "authors": "Gabriel Chua",
        "publish_date": "2025-07-25",
        "summary_en": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.",
        "summary_zh": "随着大型语言模型(LLMs)越来越多地集成原生代码解释器，它们能够实现强大的实时执行能力，大大扩展了它们的实用性。然而，这种集成引入了潜在的系统性网络安全威胁，这些威胁与基于提示的漏洞有根本不同。为了系统性地评估这些特定于解释器的风险，我们提出了CIRCLE（针对LLM漏洞的代码解释器弹性检查），这是一个包含1260个提示的简单基准测试，这些提示针对CPU、内存和磁盘资源耗尽。每个风险类别都包含明确的恶意(\"直接\")和看似良性(\"间接\")的提示变体。我们的自动化评估框架不仅评估LLM是否拒绝或生成有风险的代码，还在解释器环境中执行生成的代码，以评估代码的正确性、LLM为使代码安全所做的简化或执行超时。评估了OpenAI和Google的7个商业可用模型，我们发现存在显著且不一致的漏洞。例如，评估显示，即使在提供商内部也存在巨大差异——OpenAI的o4-mini正确拒绝风险请求的比例为7.1%，明显高于GPT-4.1的0.5%。结果特别强调，间接的、社会工程学的提示显著削弱了模型的防御能力。这突显了对特定于解释器的网络安全基准测试、专门的缓解工具（例如护栏）以及明确的行业标准的迫切需求，以指导LLM解释器集成的安全和负责任的部署。基准测试数据集和评估代码已公开发布，以促进进一步的研究。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "评估LLM代码解释器防御资源耗尽攻击的能力，包括直接和间接提示变体。",
        "analysis_time": "2025-08-04T19:55:27.369131"
    },
    {
        "id": "2507.21035",
        "title_en": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via   Code-Driven Gene Expression Analysis",
        "title_zh": "GenoMAS: 一种基于代码驱动的基因表达分析的科学发现多智能体框架",
        "url": "https://arxiv.org/abs/2507.21035",
        "authors": "Haoyang Liu, Yijiang Li, Haohan Wang",
        "publish_date": "2025-07-28",
        "summary_en": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F_1 of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
        "summary_zh": "基因表达分析是许多生物医学发现的关键，然而，由于多个大型半结构化文件的复杂性以及对广泛领域专业知识的需求，从原始转录组数据中提取见解仍然是一项艰巨任务。当前的自动化方法通常受到限制：要么是在边缘情况下失效的僵化工作流，要么是缺乏严谨科学研究所需必要精度的完全自主智能体。GenoMAS开辟了一条新路径，它呈现了一支基于大语言模型(LLM)的科学家团队，将结构化工作流的可靠性与自主智能体的适应性相结合。GenoMAS通过类型化消息传递协议协调六个专门的LLM智能体，每个智能体为共享的分析画布贡献互补的优势。GenoMAS的核心是一个引导式规划框架：编程智能体将高层任务指导展开为行动单元(Action Units)，并在每个节点选择前进、修订、绕过或回溯，从而在优雅地适应基因组数据特性的同时保持逻辑连贯性。在GenoTEX基准测试中，GenoMAS在数据预处理方面达到89.13%的复合相似性相关性，在基因识别方面达到60.48%的F1值，分别比之前的最佳方法高出10.61%和16.85%。除了指标之外，GenoMAS还揭示出文献支持的生物学合理的基因-表型关联，同时调整了潜在的混杂因素。代码可在https://github.com/Liu-Hy/GenoMAS获取。",
        "github_repo": "https://github.com/Liu-Hy/GenoMAS",
        "project_page": "暂无",
        "model_function": "基于LLM的多智能体框架，通过代码驱动的基因表达分析实现科学发现，结合结构化工作流与自主智能体的优势。",
        "analysis_time": "2025-08-04T19:55:31.307048"
    },
    {
        "id": "2507.20152",
        "title_en": "Goal Alignment in LLM-Based User Simulators for Conversational AI",
        "title_zh": "基于大型语言模型的对话AI用户模拟器中的目标对齐",
        "url": "https://arxiv.org/abs/2507.20152",
        "authors": "Shuhaib Mehri, Xiaocheng Yang, Takyoung Kim, Gokhan Tur, Shikib Mehri, Dilek Hakkani-Tür",
        "publish_date": "2025-07-27",
        "summary_en": "User simulators are essential to conversational AI, enabling scalable agent\ndevelopment and evaluation through simulated interactions. While current Large\nLanguage Models (LLMs) have advanced user simulation capabilities, we reveal\nthat they struggle to consistently demonstrate goal-oriented behavior across\nmulti-turn conversations--a critical limitation that compromises their\nreliability in downstream applications. We introduce User Goal State Tracking\n(UGST), a novel framework that tracks user goal progression throughout\nconversations. Leveraging UGST, we present a three-stage methodology for\ndeveloping user simulators that can autonomously track goal progression and\nreason to generate goal-aligned responses. Moreover, we establish comprehensive\nevaluation metrics for measuring goal alignment in user simulators, and\ndemonstrate that our approach yields substantial improvements across two\nbenchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a\ncritical gap in conversational AI and establish UGST as an essential framework\nfor developing goal-aligned user simulators.",
        "summary_zh": "用户模拟器对对话AI至关重要，它通过模拟交互实现了智能体的可扩展开发和评估。虽然当前的大型语言模型（LLMs）已经提升了用户模拟能力，但我们发现它们难以在多轮对话中持续展示目标导向行为——这一关键缺陷损害了它们在下游应用中的可靠性。我们引入了用户目标状态跟踪（UGST），这是一个新颖的框架，用于跟踪整个对话过程中用户目标的进展。利用UGST，我们提出了一个三阶段方法论，用于开发能够自主跟踪目标进展并进行推理以生成目标对齐响应的用户模拟器。此外，我们建立了全面的评估指标来衡量用户模拟器中的目标对齐，并在两个基准测试（MultiWOZ 2.4和{\\tau}-Bench）上展示了我们的方法取得了显著改进。我们的研究成果解决了对话AI中的一个关键空白，并将UGST确立为开发目标对齐用户模拟器的基本框架。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "开发能够跟踪用户目标进展并生成目标对齐响应的用户模拟器框架，用于对话AI系统的开发和评估。",
        "analysis_time": "2025-08-04T19:55:53.997016"
    }
]