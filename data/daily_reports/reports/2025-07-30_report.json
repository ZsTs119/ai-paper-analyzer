[
    {
        "id": "2507.21990",
        "title_en": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical   Knowledge",
        "title_zh": "ChemDFM-R: 一种通过原子化化学知识增强的化学推理大型语言模型",
        "url": "https://arxiv.org/abs/2507.21990",
        "authors": "Zihan Zhao, Bo Chen, Ziping Wan, Lu Chen, Xuanze Lin, Shiyang Yu, Situo Zhang, Da Ma, Zichen Zhu, Danyang Zhang, Huayang Wang, Zhongyang Dai, Liyang Wen, Xin Chen, Kai Yu",
        "publish_date": "2025-07-29",
        "summary_en": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves state-of-the-art performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
        "summary_zh": "尽管大型语言模型(LLMs)已取得了令人印象深刻的进展，但它们在化学等科学领域的应用仍受限于浅层的领域理解和有限的推理能力。在本工作中，我们专注于化学这一特定领域，开发了一个化学推理大型语言模型ChemDFM-R。我们首先构建了一个全面的原子化知识点数据集，以增强模型对化学基本原理和逻辑结构的理解。然后，我们提出了一种混合来源的蒸馏策略，该策略整合了专家策划的知识与通用领域的推理技能，随后进行特定领域的强化学习以增强化学推理能力。在多样化的化学基准测试上的实验表明，ChemDFM-R取得了最先进的性能，同时提供可解释的、基于理由的输出。进一步的案例研究说明了显式推理链如何显著提高模型在现实世界人机协作场景中的可靠性、透明度和实用价值。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "专注于化学领域的推理大模型，通过原子化知识增强化学推理能力并提供可解释输出。",
        "analysis_time": "2025-08-04T19:20:56.962940"
    },
    {
        "id": "2507.22058",
        "title_en": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image   Generative Models Great Again",
        "title_zh": "X-Omni：强化学习让离散自回归图像生成模型再次强大",
        "url": "https://arxiv.org/abs/2507.22058",
        "authors": "Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, Jie Jiang",
        "publish_date": "2025-07-29",
        "summary_en": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.",
        "summary_zh": "已经付出了大量努力将\"下一个令牌预测\"范式扩展到视觉内容，旨在为图像生成和理解创建统一的方法。然而，通过离散令牌的自回归建模尝试生成图像一直受到诸如低视觉保真度、输出失真以及渲染复杂细节时无法遵循复杂指令等问题困扰。这些缺点可能归因于自回归推理过程中的累积误差或离散化过程中造成的信息损失。可能正是因为这一挑战，最近的研究越来越多地转向联合训练图像生成（使用扩散目标）和语言生成（使用自回归目标），逐渐远离统一的建模方法。在这项工作中，我们证明了强化学习可以有效减轻伪影并显著提高离散自回归建模方法的生成质量，从而实现图像和语言生成的无缝集成。我们的框架包括一个语义图像标记器、一个用于语言和图像的统一自回归模型，以及一个用于图像生成的离线扩散解码器，称为X-Omni。X-Omni在使用7B语言模型的图像生成任务中实现了最先进的性能，生成具有高美学质量的图像，同时展现出遵循指令和渲染长文本的强大能力。",
        "github_repo": "https://github.com/X-Omni-Team/X-Omni",
        "project_page": "https://x-omni-team.github.io",
        "model_function": "结合强化学习的离散自回归图像生成模型，实现高质量图像生成与语言图像统一生成。",
        "analysis_time": "2025-08-04T19:21:00.919386"
    },
    {
        "id": "2507.20254",
        "title_en": "MIRepNet: A Pipeline and Foundation Model for EEG-Based Motor Imagery   Classification",
        "title_zh": "MIRepNet：一种基于脑电图的运动想象分类流水线与基础模型",
        "url": "https://arxiv.org/abs/2507.20254",
        "authors": "Dingkun Liu, Zhu Chen, Jingwei Luo, Shijie Lian, Dongrui Wu",
        "publish_date": "2025-07-27",
        "summary_en": "Brain-computer interfaces (BCIs) enable direct communication between the\nbrain and external devices. Recent EEG foundation models aim to learn\ngeneralized representations across diverse BCI paradigms. However, these\napproaches overlook fundamental paradigm-specific neurophysiological\ndistinctions, limiting their generalization ability. Importantly, in practical\nBCI deployments, the specific paradigm such as motor imagery (MI) for stroke\nrehabilitation or assistive robotics, is generally determined prior to data\nacquisition. This paper proposes MIRepNet, the first EEG foundation model\ntailored for the MI paradigm. MIRepNet comprises a high-quality EEG\npreprocessing pipeline incorporating a neurophysiologically-informed channel\ntemplate, adaptable to EEG headsets with arbitrary electrode configurations.\nFurthermore, we introduce a hybrid pretraining strategy that combines\nself-supervised masked token reconstruction and supervised MI classification,\nfacilitating rapid adaptation and accurate decoding on novel downstream MI\ntasks with fewer than 30 trials per class. Extensive evaluations across five\npublic MI datasets demonstrated that MIRepNet consistently achieved\nstate-of-the-art performance, significantly outperforming both specialized and\ngeneralized EEG models. Our code will be available on\nGitHubhttps://github.com/staraink/MIRepNet.",
        "summary_zh": "脑机接口(BCIs)实现大脑与外部设备之间的直接通信。最近的脑电图(EEG)基础模型旨在学习跨多种BCI范式的通用表征。然而，这些方法忽视了范式特定的基本神经生理学差异，限制了它们的泛化能力。重要的是，在实际的BCI部署中，特定的范式(如用于中风康复或辅助机器人的运动想象(MI))通常在数据采集前就已确定。本文提出了MIRepNet，这是首个专为MI范式定制的EEG基础模型。MIRepNet包含高质量的EEG预处理流水线，该流水线融入了神经生理学信息引导的通道模板，可适应具有任意电极配置的EEG头戴设备。此外，我们引入了一种混合预训练策略，结合了自监督的掩码令牌重建和监督式MI分类，使得在新型下游MI任务上(每类少于30次试验)能够快速适应和准确解码。在五个公共MI数据集上的广泛评估表明，MIRepNet始终取得了最先进的性能，显著优于专用和通用EEG模型。我们的代码将在GitHub上提供：https://github.com/staraink/MIRepNet。",
        "github_repo": "https://github.com/staraink/MIRepNet",
        "project_page": "暂无",
        "model_function": "专为运动想象范式设计的EEG基础模型，包含高质量预处理流水线和混合预训练策略，实现快速适应和准确解码。",
        "analysis_time": "2025-08-04T19:21:01.241871"
    },
    {
        "id": "2507.21809",
        "title_en": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D   Worlds from Words or Pixels",
        "title_zh": "浑元世界1.0：从文字或像素生成沉浸式、可探索和交互式的3D世界",
        "url": "https://arxiv.org/abs/2507.21809",
        "authors": "HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, Yihang Lian, Yulin Tsai, Lifu Wang, Sicong Liu, Puhua Jiang, Xianghui Yang, Dongyuan Guo, Yixuan Tang, Xinyue Mao, Jiaao Yu, Junlin Yu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Chao Zhang, Yonghao Tan, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Minghui Chen, Zhan Li, Wangchen Qin, Lei Wang, Yifu Sun, Lin Niu, Xiang Yuan, Xiaofeng Yang, Yingping He, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Tian Liu, Peng Chen, Di Wang, Yuhong Liu, Linus, Jie Jiang, Tengfei Wang, Chunchao Guo",
        "publish_date": "2025-07-29",
        "summary_en": "Creating immersive and playable 3D worlds from texts or images remains a\nfundamental challenge in computer vision and graphics. Existing world\ngeneration approaches typically fall into two categories: video-based methods\nthat offer rich diversity but lack 3D consistency and rendering efficiency, and\n3D-based methods that provide geometric consistency but struggle with limited\ntraining data and memory-inefficient representations. To address these\nlimitations, we present HunyuanWorld 1.0, a novel framework that combines the\nbest of both worlds for generating immersive, explorable, and interactive 3D\nscenes from text and image conditions. Our approach features three key\nadvantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2)\nmesh export capabilities for seamless compatibility with existing computer\ngraphics pipelines; 3) disentangled object representations for augmented\ninteractivity. The core of our framework is a semantically layered 3D mesh\nrepresentation that leverages panoramic images as 360{\\deg} world proxies for\nsemantic-aware world decomposition and reconstruction, enabling the generation\nof diverse 3D worlds. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in generating coherent, explorable, and\ninteractive 3D worlds while enabling versatile applications in virtual reality,\nphysical simulation, game development, and interactive content creation.",
        "summary_zh": "从文本或图像创建沉浸式和可玩的3D世界仍然是计算机视觉和图形学中的一个基本挑战。现有的世界生成方法通常分为两类：基于视频的方法提供了丰富的多样性，但缺乏3D一致性和渲染效率；而基于3D的方法提供了几何一致性，但面临着训练数据有限和内存效率低下的表示方式的挑战。为了解决这些局限性，我们提出了浑元世界1.0，这是一个新颖的框架，结合了两种方法的优势，从文本和图像条件生成沉浸式、可探索和交互式的3D场景。我们的方法具有三个关键优势：1) 通过全景世界代理提供360度沉浸式体验；2) 网格导出功能，与现有计算机图形管线无缝兼容；3) 解耦的对象表示，增强交互性。我们框架的核心是一种语义分层的3D网格表示，它利用全景图像作为360度世界代理，进行语义感知的世界分解和重建，从而能够生成多样化的3D世界。大量实验表明，我们的方法在生成连贯、可探索和交互式的3D世界方面取得了最先进的性能，同时在虚拟现实、物理模拟、游戏开发和交互式内容创作等应用中展现了广泛的用途。",
        "github_repo": "https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0",
        "project_page": "https://3d-models.hunyuan.tencent.com/world/",
        "model_function": "从文本或图像生成沉浸式、可探索和交互式3D世界，支持全景体验与图形管线兼容。",
        "analysis_time": "2025-08-04T19:21:02.765710"
    },
    {
        "id": "2507.21183",
        "title_en": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
        "title_zh": "MaPPO: 结合先验知识的最大后验偏好优化",
        "url": "https://arxiv.org/abs/2507.21183",
        "authors": "Guangchen Lan, Sipeng Zhang, Tianle Wang, Yuwei Zhang, Daoan Zhang, Xinpeng Wei, Xiaoman Pan, Hongming Zhang, Dong-Jun Han, Christopher G. Brinton",
        "publish_date": "2025-07-27",
        "summary_en": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency.",
        "summary_zh": "随着代表用户的大型语言模型(LLMs)时代的展开，偏好优化(PO)方法已成为使LLMs与人类偏好保持一致并提高性能的核心方法。我们提出了最大后验偏好优化(MaPPO)，这是一个从偏好中学习的框架，明确地将先验奖励知识整合到优化目标中。虽然现有的方法如直接偏好优化(DPO)及其变体将偏好学习视为最大似然估计(MLE)问题，但MaPPO通过将先验奖励估计整合到有原则的最大后验(MaP)目标中扩展了这一范式。这不仅推广了DPO及其变体，还通过减轻对响应的过度简化二元分类来增强对齐。更重要的是，MaPPO不引入额外的超参数，并支持离线和在线设置下的偏好优化。此外，MaPPO可以作为插件使用，对DPO变体(包括广泛使用的SimPO、IPO和CPO)提供一致的改进。在不同模型大小和模型系列上的广泛经验评估，包括在MT-Bench、AlpacaEval 2.0和Arena-Hard三个标准基准测试上，证明了在不牺牲计算效率的情况下对齐性能的一致提升。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "结合先验知识的偏好优化方法，提升LLMs与人类偏好对齐，无需额外超参数支持离线和在线场景。",
        "analysis_time": "2025-08-04T19:21:04.733103"
    },
    {
        "id": "2507.14111",
        "title_en": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement   Learning",
        "title_zh": "CUDA-L1：通过对比强化学习改进CUDA优化",
        "url": "https://arxiv.org/abs/2507.14111",
        "authors": "Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum",
        "publish_date": "2025-07-18",
        "summary_en": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
        "summary_zh": "由大型语言模型的快速发展驱动的GPU计算资源需求呈指数级增长，这迫切需要自动化的CUDA优化策略。尽管最近LLMs的进展在代码生成方面显示出前景，但当前最先进的模型（如R1、o1）在提高CUDA速度方面的成功率较低。在本文中，我们介绍了CUDA-L1，一个用于CUDA优化的自动化强化学习框架。CUDA-L1在CUDA优化任务上实现了性能提升：在NVIDIA A100上训练后，它在KernelBench的所有250个CUDA内核上实现了平均17.7倍的加速，峰值加速达到449倍。此外，尽管该模型专门针对A100进行了优化，但它还显示出在GPU架构之间的卓越可移植性，在H100上实现了17.8倍的平均加速，在RTX 3090上实现了19.0倍，在L40上实现了16.5倍，在H800上实现了14.7倍，在H20上实现了13.9倍。除了这些基准测试结果外，CUDA-L1还展示了几个显著特性：1) 发现了多种CUDA优化技术，并学会了战略性地组合它们以实现最佳性能；2) 揭示了CUDA优化的基本原理；3) 识别出非明显的性能瓶颈，并拒绝那些看似有益但实际有害的优化。CUDA-L1的能力表明，仅通过基于加速的奖励信号，强化学习可以将最初表现不佳的LLM转变为有效的CUDA优化器，而无需人类专业知识或领域知识。更重要的是，训练好的RL模型将获得的推理能力扩展到新的内核。这种范式为CUDA操作的自动化优化开辟了可能性，有望显著提高GPU效率并缓解GPU计算资源日益增长的压力。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "自动化CUDA优化框架，通过强化学习发现并组合优化技术，实现显著性能提升和跨架构可移植性。",
        "analysis_time": "2025-08-04T19:21:06.069696"
    },
    {
        "id": "2507.22061",
        "title_en": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
        "title_zh": "MOVE：运动引导的小样本视频目标分割",
        "url": "https://arxiv.org/abs/2507.22061",
        "authors": "Kaining Ying, Hengrui Hu, Henghui Ding",
        "publish_date": "2025-07-29",
        "summary_en": "This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.",
        "summary_zh": "这项工作解决了运动引导的小样本视频目标分割(FSVOS)问题，旨在基于具有相同运动模式的少量标注示例来分割视频中的动态对象。现有的FSVOS数据集和方法通常关注对象类别，这是静态属性，忽略了视频中的丰富时间动态性，限制了它们在需要运动理解的应用场景中的应用。为了填补这一空白，我们引入了MOVE，这是一个专为运动引导的FSVOS设计的大规模数据集。基于MOVE，我们在两种实验设置下全面评估了来自3个不同相关任务的6种最先进方法。我们的结果表明，当前方法难以解决运动引导的FSVOS问题，促使我们分析相关挑战并提出一种基线方法——解耦运动外观网络(DMA)。实验证明，我们的方法在少样本运动理解方面取得了优越的性能，为这一方向的未来研究奠定了坚实的基础。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "基于少量样本和运动模式引导的视频动态目标分割",
        "analysis_time": "2025-08-04T19:21:06.957441"
    },
    {
        "id": "2507.21364",
        "title_en": "Evaluating Deep Learning Models for African Wildlife Image   Classification: From DenseNet to Vision Transformers",
        "title_zh": "评估深度学习模型在非洲野生动物图像分类中的应用：从DenseNet到Vision Transformers",
        "url": "https://arxiv.org/abs/2507.21364",
        "authors": "Lukman Jibril Aliyu, Umar Sani Muhammad, Bilqisu Ismail, Nasiru Muhammad, Almustapha A Wakili, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, Mustapha Abdullahi",
        "publish_date": "2025-07-28",
        "summary_en": "Wildlife populations in Africa face severe threats, with vertebrate numbers\ndeclining by over 65% in the past five decades. In response, image\nclassification using deep learning has emerged as a promising tool for\nbiodiversity monitoring and conservation. This paper presents a comparative\nstudy of deep learning models for automatically classifying African wildlife\nimages, focusing on transfer learning with frozen feature extractors. Using a\npublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; we\nevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and\nVision Transformer ViT-H/14. DenseNet-201 achieved the best performance among\nconvolutional networks (67% accuracy), while ViT-H/14 achieved the highest\noverall accuracy (99%), but with significantly higher computational cost,\nraising deployment concerns. Our experiments highlight the trade-offs between\naccuracy, resource requirements, and deployability. The best-performing CNN\n(DenseNet-201) was integrated into a HF Mirror Gradio Space for real-time\nfield use, demonstrating the feasibility of deploying lightweight models in\nconservation settings. This work contributes to African-grounded AI research by\noffering practical insights into model selection, dataset preparation, and\nresponsible deployment of deep learning tools for wildlife conservation.",
        "summary_zh": "非洲的野生动物种群面临严重威胁，在过去五十年中，脊椎动物数量下降了65%以上。作为回应，使用深度学习的图像分类已成为生物多样性监测和保护的有力工具。本文提出了一项深度学习模型的比较研究，用于自动分类非洲野生动物图像，重点关注带有冻结特征提取器的迁移学习。我们使用了一个包含四种物种（水牛、大象、犀牛和斑马）的公共数据集，评估了DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14的性能。在卷积网络中，DenseNet-201取得了最佳性能（67%的准确率），而ViT-H/14取得了最高的整体准确率（99%），但计算成本显著更高，引发了部署方面的担忧。我们的实验突显了准确性、资源需求和可部署性之间的权衡。表现最佳的CNN（DenseNet-201）被整合到一个HF Mirror Gradio Space中，用于实时野外使用，展示了在保护环境中部署轻量级模型的可行性。这项工作通过提供关于模型选择、数据集准备和深度学习工具在野生动物保护中的负责任部署的实际见解，为非洲本土AI研究做出了贡献。",
        "github_repo": "暂无",
        "project_page": "暂无",
        "model_function": "评估深度学习模型在非洲野生动物图像分类中的性能，并提供模型选择和部署的实用指导。",
        "analysis_time": "2025-08-04T19:21:09.197251"
    },
    {
        "id": "2507.20240",
        "title_en": "AnimalClue: Recognizing Animals by their Traces",
        "title_zh": "AnimalClue: 通过动物痕迹识别动物",
        "url": "https://arxiv.org/abs/2507.20240",
        "authors": "Risa Shinoda, Nakamasa Inoue, Iro Laina, Christian Rupprecht, Hirokatsu Kataoka",
        "publish_date": "2025-07-27",
        "summary_en": "Wildlife observation plays an important role in biodiversity conservation,\nnecessitating robust methodologies for monitoring wildlife populations and\ninterspecies interactions. Recent advances in computer vision have\nsignificantly contributed to automating fundamental wildlife observation tasks,\nsuch as animal detection and species identification. However, accurately\nidentifying species from indirect evidence like footprints and feces remains\nrelatively underexplored, despite its importance in contributing to wildlife\nmonitoring. To bridge this gap, we introduce AnimalClue, the first large-scale\ndataset for species identification from images of indirect evidence. Our\ndataset consists of 159,605 bounding boxes encompassing five categories of\nindirect clues: footprints, feces, eggs, bones, and feathers. It covers 968\nspecies, 200 families, and 65 orders. Each image is annotated with\nspecies-level labels, bounding boxes or segmentation masks, and fine-grained\ntrait information, including activity patterns and habitat preferences. Unlike\nexisting datasets primarily focused on direct visual features (e.g., animal\nappearances), AnimalClue presents unique challenges for classification,\ndetection, and instance segmentation tasks due to the need for recognizing more\ndetailed and subtle visual features. In our experiments, we extensively\nevaluate representative vision models and identify key challenges in animal\nidentification from their traces. Our dataset and code are available at\nhttps://dahlian00.github.io/AnimalCluePage/",
        "summary_zh": "野生动物观察在生物多样性保护中发挥着重要作用，需要强大的方法来监测野生动物种群和物种间相互作用。计算机视觉的最新进展极大地促进了基本野生动物观察任务的自动化，如动物检测和物种识别。然而，尽管从脚印和粪便等间接证据准确识别物种对野生动物监测很重要，但这一领域仍然相对未被充分探索。为了填补这一空白，我们介绍了AnimalClue，这是第一个用于从间接证据图像中进行物种识别的大规模数据集。我们的数据集包含159,605个边界框，涵盖五类间接线索：脚印、粪便、蛋、骨头和羽毛。它覆盖了968个物种、200个科和65个目。每张图像都标注了物种级别的标签、边界框或分割掩码，以及细粒度的特征信息，包括活动模式和栖息地偏好。与主要关注直接视觉特征（如动物外观）的现有数据集不同，由于需要识别更详细和微妙的视觉特征，AnimalClue为分类、检测和实例分割任务带来了独特的挑战。在我们的实验中，我们广泛评估了代表性的视觉模型，并确定了从动物痕迹中识别动物的关键挑战。我们的数据集和代码可在https://dahlian00.github.io/AnimalCluePage/获取",
        "github_repo": "https://github.com/dahlian00/AnimalClue",
        "project_page": "https://dahlian00.github.io/AnimalCluePage/",
        "model_function": "通过动物痕迹图像识别物种，为野生动物监测提供自动化工具",
        "analysis_time": "2025-08-04T19:21:12.440282"
    },
    {
        "id": "2507.21503",
        "title_en": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via   Unanswerable Visual Questions",
        "title_zh": "MoHoBench：通过不可回答的视觉问题评估多模态大语言模型的诚实性",
        "url": "https://arxiv.org/abs/2507.21503",
        "authors": "Yanxu Zhu, Shitong Duan, Xiangxu Zhang, Jitao Sang, Peng Zhang, Tun Lu, Xiao Zhou, Jing Yao, Xiaoyuan Yi, Xing Xie",
        "publish_date": "2025-07-29",
        "summary_en": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable\nadvancements in vision-language tasks, yet produce potentially harmful or\nuntrustworthy content. Despite substantial work investigating the\ntrustworthiness of language models, MMLMs' capability to act honestly,\nespecially when faced with visually unanswerable questions, remains largely\nunderexplored. This work presents the first systematic assessment of honesty\nbehaviors across various MLLMs. We ground honesty in models' response behaviors\nto unanswerable visual questions, define four representative types of such\nquestions, and construct MoHoBench, a large-scale MMLM honest benchmark,\nconsisting of 12k+ visual question samples, whose quality is guaranteed by\nmulti-stage filtering and human verification. Using MoHoBench, we benchmarked\nthe honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our\nfindings show that: (1) most models fail to appropriately refuse to answer when\nnecessary, and (2) MMLMs' honesty is not solely a language modeling issue, but\nis deeply influenced by visual information, necessitating the development of\ndedicated methods for multimodal honesty alignment. Therefore, we implemented\ninitial alignment methods using supervised and preference learning to improve\nhonesty behavior, providing a foundation for future work on trustworthy MLLMs.\nOur data and code can be found at https://github.com/DSTTSD/MoHoBench.",
        "summary_zh": "最近，多模态大语言模型(MLLMs)在视觉语言任务方面取得了显著进展，但可能产生有害或不可信的内容。尽管有大量工作研究语言模型的可信度，但MLLMs在面临视觉不可回答问题时表现出的诚实能力在很大程度上仍未得到充分探索。这项工作首次对各种MLLMs的诚实行为进行了系统性评估。我们将诚实性基于模型对不可回答视觉问题的响应行为，定义了四种代表性的此类问题，并构建了MoHoBench，一个大规模的MLLM诚实性基准，包含12k+个视觉问题样本，其质量通过多阶段过滤和人工验证得到保证。使用MoHoBench，我们对28个流行的MLLMs的诚实性进行了基准测试，并进行了全面分析。我们的发现显示：(1)大多数模型在必要时未能适当拒绝回答，以及(2)MLLMs的诚实性不仅仅是一个语言建模问题，而是深受视觉信息的影响，这需要开发专门的多模态诚实性对齐方法。因此，我们使用监督学习和偏好学习实施了初步的对齐方法，以改善诚实性行为，为未来可信MLLMs的工作奠定了基础。我们的数据和代码可以在https://github.com/DSTTSD/MoHoBench找到。",
        "github_repo": "https://github.com/DSTTSD/MoHoBench",
        "project_page": "暂无",
        "model_function": "评估多模态大语言模型面对视觉不可回答问题时的诚实性，提高模型可靠性。",
        "analysis_time": "2025-08-04T19:21:15.887443"
    }
]