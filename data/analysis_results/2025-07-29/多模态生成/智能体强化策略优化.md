# 文本生成

# 智能体强化策略优化

**论文ID**：2507.19849
**英文标题**：Agentic Reinforced Policy Optimization
**中文标题**：智能体强化策略优化
**论文地址**：https://arxiv.org/abs/2507.19849

**作者团队**：Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou
**发表日期**：2025-07-26

**英文摘要**：
Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO

**中文摘要**：
具有可验证奖励的大规模强化学习(RLVR)已被证明在利用大型语言模型(LLMs)进行单回合推理任务方面是有效的。在实际推理场景中，LLMs通常可以利用外部工具来协助任务解决过程。然而，当前的RL算法无法充分平衡模型的内在长程推理能力和其在多回合工具交互中的熟练度。为了弥合这一差距，我们提出了智能体强化策略优化(ARPO)，这是一种专门用于训练多回合基于LLMs的智能体的新型智能体RL算法。通过初步实验，我们观察到LLMs在与外部工具交互后往往会表现出高度不确定的行为，表现为生成令牌的熵分布增加。受这一观察结果的启发，ARPO纳入了一种基于熵的自适应回滚机制，动态平衡全局轨迹采样和步骤级采样，从而促进工具使用后在高度不确定步骤的探索。通过整合优势归因估计，ARPO使LLMs能够内化工具使用交互中的步骤级优势差异。我们在计算推理、知识推理和深度搜索领域的13个具有挑战性的基准测试中的实验表明，ARPO优于轨迹级RL算法。值得注意的是，ARPO仅使用现有方法所需工具使用预算的一半就实现了性能提升，为将基于LLMs的智能体与实时动态环境对齐提供了可扩展的解决方案。我们的代码和数据集已在 https://github.com/dongguanting/ARPO 发布。

**GitHub仓库**：https://github.com/dongguanting/ARPO
**项目页面**：https://github.com/dongguanting/ARPO
**模型功能**：一种基于熵的自适应回滚机制，用于训练多回合LLM智能体，平衡长程推理与多工具交互能力，优化工具使用效率。

**技术特点**：ARPO引入了基于熵的自适应回滚机制，动态平衡全局轨迹采样和步骤级采样，特别针对LLMs在工具交互后出现的高不确定性步骤进行优化；同时整合优势归因估计，使模型能够内化工具使用交互中的步骤级优势差异，显著提高了工具使用效率。

**应用场景**：复杂计算推理任务，如数学问题求解和编程辅助；知识密集型问答系统，需要整合多种工具和知识来源；深度搜索应用，如搜索引擎优化和信息检索排序等需要高效利用工具的场景。

**分析时间**：2025-08-04T19:54:48.586784