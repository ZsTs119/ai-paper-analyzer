# 文本生成

# 超越二元奖励：训练语言模型对其不确定性进行推理

**论文ID**：2507.16806
**英文标题**：Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty
**中文标题**：超越二元奖励：训练语言模型对其不确定性进行推理
**论文地址**：https://arxiv.org/abs/2507.16806

**作者团队**：Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas
**发表日期**：2025-07-22

**英文摘要**：
When language models (LMs) are trained via reinforcement learning (RL) to
generate natural language "reasoning chains", their performance improves on a
variety of difficult question answering tasks. Today, almost all successful
applications of RL for reasoning use binary reward functions that evaluate the
correctness of LM outputs. Because such reward functions do not penalize
guessing or low-confidence outputs, they often have the unintended side-effect
of degrading calibration and increasing the rate at which LMs generate
incorrect responses (or "hallucinate") in other problem domains. This paper
describes RLCR (Reinforcement Learning with Calibration Rewards), an approach
to training reasoning models that jointly improves accuracy and calibrated
confidence estimation. During RLCR, LMs generate both predictions and numerical
confidence estimates after reasoning. They are trained to optimize a reward
function that augments a binary correctness score with a Brier score -- a
scoring rule for confidence estimates that incentivizes calibrated prediction.
We first prove that this reward function (or any analogous reward function that
uses a bounded, proper scoring rule) yields models whose predictions are both
accurate and well-calibrated. We next show that across diverse datasets, RLCR
substantially improves calibration with no loss in accuracy, on both in-domain
and out-of-domain evaluations -- outperforming both ordinary RL training and
classifiers trained to assign post-hoc confidence scores. While ordinary RL
hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized
confidence can be leveraged at test time to improve accuracy and calibration
via confidence-weighted scaling methods. Our results show that explicitly
optimizing for calibration can produce more generally reliable reasoning
models.

**中文摘要**：
当语言模型(LMs)通过强化学习(RL)训练以生成自然语言"推理链"时，它们在各种困难的问答任务上表现更好。如今，几乎所有用于推理的成功RL应用都使用二元奖励函数来评估LM输出的正确性。由于这类奖励函数不会惩罚猜测或低置信度的输出，它们常常会产生意外的副作用，降低校准能力，并增加LM在其他问题领域中生成不正确响应(或"产生幻觉")的速率。本文描述了RLCR(带校准奖励的强化学习)，这是一种训练推理模型的方法，可以同时提高准确性和校准置信度估计。在RLCR过程中，LM在推理后生成预测和数值置信度估计。它们被训练以优化奖励函数，该函数将二元正确性分数与Brier分数(一种置信度估计的评分规则)相结合，该规则激励校准预测。我们首先证明，这种奖励函数(或任何使用有界、适当评分规则的类似奖励函数)产生的模型，其预测既准确又具有良好的校准性。接下来，我们展示在各种数据集上，RLCR在领域内和领域外评估中显著提高了校准能力，同时没有损失准确性，优于普通RL训练和分配事后置信度分数的分类器。虽然普通RL会损害校准能力，但RLCR却能改善它。最后，我们证明在测试时可以利用语言化的置信度，通过置信度加权缩放方法来提高准确性和校准能力。我们的结果表明，明确优化校准可以产生更普遍可靠的推理模型。

**GitHub仓库**：暂无
**项目页面**：暂无
**模型功能**：训练语言模型生成带有校准置信度估计的推理链，提高预测准确性和可靠性

**技术特点**：RLCR方法结合二元正确性分数与Brier分数作为奖励函数，激励模型生成校准预测；通过理论证明和实验验证，该方法在各种数据集上显著提高了校准能力同时保持准确性，优于普通RL训练和事后置信度分类器；支持在测试时利用语言化置信度进行置信度加权缩放，进一步提升模型性能。

**应用场景**：高可靠性问答系统，需要提供准确答案并给出适当置信度估计；决策支持系统，需要模型提供预测结果并明确表达不确定性；教育辅导系统，向学生解释推理过程并指出不确定性的智能辅导应用。

**分析时间**：2025-08-04T19:55:20.006729