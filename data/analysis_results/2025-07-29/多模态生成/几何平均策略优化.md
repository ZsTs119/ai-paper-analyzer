# 文本生成

# 几何平均策略优化

**论文ID**：2507.20673
**英文标题**：Geometric-Mean Policy Optimization
**中文标题**：几何平均策略优化
**论文地址**：https://arxiv.org/abs/2507.20673

**作者团队**：Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei
**发表日期**：2025-07-28

**英文摘要**：
Recent advancements, such as Group Relative Policy Optimization (GRPO), have
enhanced the reasoning capabilities of large language models by optimizing the
arithmetic mean of token-level rewards. However, GRPO suffers from unstable
policy updates when processing tokens with outlier importance-weighted rewards,
which manifests as extreme importance sampling ratios during training, i.e.,
the ratio between the sampling probabilities assigned to a token by the current
and old policies. In this work, we propose Geometric-Mean Policy Optimization
(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic
mean, GMPO maximizes the geometric mean of token-level rewards, which is
inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical
and experimental analysis to justify the design and stability benefits of GMPO.
Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on
multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,
including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is
available at https://github.com/callsys/GMPO.

**中文摘要**：
最近的进展，如组相对策略优化（GRPO），通过优化标记级奖励的算术平均值，增强了大型语言模型的推理能力。然而，GRPO在处理具有异常值重要性加权奖励的标记时，会遇到不稳定的策略更新问题，这表现为训练期间极端的重要性采样比率，即当前策略和旧策略分配给标记的采样概率之间的比率。在这项工作中，我们提出了几何平均策略优化（GMPO），这是GRPO的一个稳定变体。GMPO不是优化算术平均值，而是最大化标记级奖励的几何平均值，这本质上对异常值不那么敏感，并保持更稳定的重要性采样比率范围。此外，我们提供了全面的理论和实验分析，以证明GMPO的设计和稳定性优势。除了提高稳定性外，GMPO-7B在多个数学基准测试上的平均表现比GRPO高出4.1%，在多模态推理基准测试上高出1.4%，包括AIME24、AMC、MATH500、OlympiadBench、Minerva和Geometry3K。代码可在https://github.com/callsys/GMPO获取。

**GitHub仓库**：https://github.com/callsys/GMPO
**项目页面**：暂无
**模型功能**：基于几何平均的奖励优化，提升语言模型推理能力与稳定性

**技术特点**：GMPO采用几何平均代替算术平均来优化标记级奖励，有效降低了模型对异常值奖励的敏感性；通过保持更稳定的重要性采样比率范围，解决了GRPO训练过程中策略更新不稳定的问题，显著提升了模型在复杂推理任务中的表现。

**应用场景**：数学竞赛题目求解系统，如AIME、AMC等数学竞赛题目的自动解答；多模态推理平台，结合文本和图像进行综合推理分析；高精度教育辅助工具，为数学学习提供详细解题步骤和推理过程。

**分析时间**：2025-08-04T19:54:59.420169