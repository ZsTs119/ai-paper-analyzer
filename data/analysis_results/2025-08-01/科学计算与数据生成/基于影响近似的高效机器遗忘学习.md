# 基于影响近似的高效机器遗忘学习

**论文ID**：2507.23257
**英文标题**：Efficient Machine Unlearning via Influence Approximation
**中文标题**：基于影响近似的高效机器遗忘学习
**论文地址**：https://arxiv.org/abs/2507.23257

**作者团队**：Jiawei Liu, Chenwang Wu, Defu Lian, Enhong Chen
**发表日期**：2025-07-31

**英文摘要**：
Due to growing privacy concerns, machine unlearning, which aims at enabling
machine learning models to ``forget" specific training data, has received
increasing attention. Among existing methods, influence-based unlearning has
emerged as a prominent approach due to its ability to estimate the impact of
individual training samples on model parameters without retraining. However,
this approach suffers from prohibitive computational overhead arising from the
necessity to compute the Hessian matrix and its inverse across all training
samples and parameters, rendering it impractical for large-scale models and
scenarios involving frequent data deletion requests. This highlights the
difficulty of forgetting. Inspired by cognitive science, which suggests that
memorizing is easier than forgetting, this paper establishes a theoretical link
between memorizing (incremental learning) and forgetting (unlearning). This
connection allows machine unlearning to be addressed from the perspective of
incremental learning. Unlike the time-consuming Hessian computations in
unlearning (forgetting), incremental learning (memorizing) typically relies on
more efficient gradient optimization, which supports the aforementioned
cognitive theory. Based on this connection, we introduce the Influence
Approximation Unlearning (IAU) algorithm for efficient machine unlearning from
the incremental perspective. Extensive empirical evaluations demonstrate that
IAU achieves a superior balance among removal guarantee, unlearning efficiency,
and comparable model utility, while outperforming state-of-the-art methods
across diverse datasets and model architectures. Our code is available at
https://github.com/Lolo1222/IAU.

**中文摘要**：
随着隐私问题的日益增长，旨在使机器学习模型能够"忘记"特定训练数据的机器遗忘学习技术受到了越来越多的关注。在现有方法中，基于影响的遗忘学习已成为一种突出方法，因为它能够在不重新训练的情况下估计单个训练样本对模型参数的影响。然而，这种方法需要计算所有训练样本和参数的Hessian矩阵及其逆矩阵，导致计算开销过大，使其对于大规模模型和涉及频繁数据删除请求的场景不切实际。这凸显了遗忘的难度。受认知科学的启发——认知科学表明记忆比遗忘更容易，本文建立了记忆（增量学习）与遗忘（遗忘学习）之间的理论联系。这种联系使得可以从增量学习的角度解决机器遗忘学习问题。与遗忘学习（遗忘）中耗时的Hessian计算不同，增量学习（记忆）通常依赖于更高效的梯度优化，这支持了上述认知理论。基于这种联系，我们提出了影响近似遗忘学习（IAU）算法，从增量学习的角度实现高效的机器遗忘学习。大量的经验评估表明，IAU在移除保证、遗忘效率和可比的模型效用之间取得了优越的平衡，并且在各种数据集和模型架构上优于最先进的方法。我们的代码可在 https://github.com/Lolo1222/IAU 获取。

**GitHub仓库**：暂无
**项目页面**：暂无
**模型功能**：高效实现机器遗忘学习，通过影响近似算法平衡移除保证、效率和模型效用。

**技术特点**：建立了记忆(增量学习)与遗忘(遗忘学习)之间的理论联系，从增量学习角度解决机器遗忘问题；提出影响近似遗忘学习(IAU)算法，避免传统方法中耗时的Hessian矩阵计算，转而使用更高效的梯度优化；在移除保证、遗忘效率和模型效用之间取得了优越的平衡。

**应用场景**：隐私保护场景：当用户要求从已训练的模型中删除其个人数据时，可以高效地实现"被遗忘权"；大规模机器学习系统：需要频繁更新数据集的场景，如推荐系统、搜索引擎等；合规性要求高的领域：如金融、医疗等需要满足数据隐私法规的行业。

**分析时间**：2025-08-04T17:50:36.792526