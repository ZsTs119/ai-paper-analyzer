# MOVE：运动引导的小样本视频目标分割

**论文ID**：2507.22061
**英文标题**：MOVE: Motion-Guided Few-Shot Video Object Segmentation
**中文标题**：MOVE：运动引导的小样本视频目标分割
**论文地址**：https://arxiv.org/abs/2507.22061

**作者团队**：Kaining Ying, Hengrui Hu, Henghui Ding
**发表日期**：2025-07-29

**英文摘要**：
This work addresses motion-guided few-shot video object segmentation (FSVOS),
which aims to segment dynamic objects in videos based on a few annotated
examples with the same motion patterns. Existing FSVOS datasets and methods
typically focus on object categories, which are static attributes that ignore
the rich temporal dynamics in videos, limiting their application in scenarios
requiring motion understanding. To fill this gap, we introduce MOVE, a
large-scale dataset specifically designed for motion-guided FSVOS. Based on
MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different
related tasks across 2 experimental settings. Our results reveal that current
methods struggle to address motion-guided FSVOS, prompting us to analyze the
associated challenges and propose a baseline method, Decoupled Motion
Appearance Network (DMA). Experiments demonstrate that our approach achieves
superior performance in few shot motion understanding, establishing a solid
foundation for future research in this direction.

**中文摘要**：
这项工作解决了运动引导的小样本视频目标分割(FSVOS)问题，旨在基于具有相同运动模式的少量标注示例来分割视频中的动态对象。现有的FSVOS数据集和方法通常关注对象类别，这是静态属性，忽略了视频中的丰富时间动态性，限制了它们在需要运动理解的应用场景中的应用。为了填补这一空白，我们引入了MOVE，这是一个专为运动引导的FSVOS设计的大规模数据集。基于MOVE，我们在两种实验设置下全面评估了来自3个不同相关任务的6种最先进方法。我们的结果表明，当前方法难以解决运动引导的FSVOS问题，促使我们分析相关挑战并提出一种基线方法——解耦运动外观网络(DMA)。实验证明，我们的方法在少样本运动理解方面取得了优越的性能，为这一方向的未来研究奠定了坚实的基础。

**GitHub仓库**：暂无
**项目页面**：暂无
**模型功能**：基于少量样本和运动模式引导的视频动态目标分割

**分析时间**：2025-08-04T19:21:06.957441
