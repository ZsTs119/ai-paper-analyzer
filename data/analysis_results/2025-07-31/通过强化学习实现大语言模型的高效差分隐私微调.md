# 通过强化学习实现大语言模型的高效差分隐私微调

**论文ID**：2507.22565
**英文标题**：Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement   Learning
**中文标题**：通过强化学习实现大语言模型的高效差分隐私微调
**论文地址**：https://arxiv.org/abs/2507.22565

**作者团队**：Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani, Gilbert Fridgen
**发表日期**：2025-07-30

**英文摘要**：
The tension between data privacy and model utility has become the defining
bottleneck for the practical deployment of large language models (LLMs) trained
on sensitive corpora including healthcare. Differentially private stochastic
gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a
pronounced cost: gradients are forcibly clipped and perturbed with noise,
degrading sample efficiency and final accuracy. Numerous variants have been
proposed to soften this trade-off, but they all share a handicap: their control
knobs are hard-coded, global, and oblivious to the evolving optimization
landscape. Consequently, practitioners are forced either to over-spend privacy
budget in pursuit of utility, or to accept mediocre models in order to stay
within privacy constraints. We present RLDP, the first framework to cast DP
optimization itself as a closed-loop control problem amenable to modern deep
reinforcement learning (RL). RLDP continuously senses rich statistics of the
learning dynamics and acts by selecting fine-grained per parameter
gradient-clipping thresholds as well as the magnitude of injected Gaussian
noise. A soft actor-critic (SAC) hyper-policy is trained online during language
model fine-tuning; it learns, from scratch, how to allocate the privacy budget
where it matters and when it matters. Across more than 1,600 ablation
experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers
perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream
utility gain. RLDP reaches each baseline's final utility after only 13-43% of
the gradient-update budget (mean speed-up 71%), all while honoring the same
(epsilon, delta)-DP contract and exhibiting equal or lower susceptibility
to membership-inference and canary-extraction attacks.

**中文摘要**：
数据隐私与模型效用之间的紧张关系已成为在敏感语料库（包括医疗领域）上训练的大语言模型（LLMs）实际部署的关键瓶颈。差分随机梯度下降（DP-SGD）能提供正式的隐私保证，但代价高昂：梯度被强制裁剪并用噪声扰动，降低了样本效率和最终准确性。虽然已有多种方法试图缓和这种权衡，但它们都存在一个共同缺陷：其控制参数是硬编码的、全局的，并且对不断变化的优化景观一无所知。因此，实践者要么为了追求效用而过度消耗隐私预算，要么为了满足隐私约束而接受平庸的模型。我们提出了RLDP，这是首个将差分隐私优化本身作为闭环控制问题的框架，适用于现代深度强化学习（RL）。RLDP持续感知学习动态的丰富统计数据，并通过选择细粒度的每参数梯度裁剪阈值和注入的高斯噪声幅度来采取行动。在语言模型微调过程中，一个软演员-评论家（SAC）超策略在线训练；它从零开始学习如何以及何时在关键之处分配隐私预算。在GPT2-small、Llama-1B、Llama-3B和Mistral-7B上进行超过1,600次消融实验，RLDP实现了1.3-30.5%（平均5.4%）的困惑度降低和平均5.6%的下游效用提升。RLDP仅需使用13-43%（平均加速71%）的梯度更新预算即可达到每个基线的最终效用，同时遵守相同的（epsilon, delta）差分隐私合约，并且对成员推断和金丝雀提取攻击的敏感性等于或低于基线。

**GitHub仓库**：https://github.com/akhadangi/RLDP
**项目页面**：https://wandb.ai/afshin-khadangi-university-of-luxembourg/RLDP/reports/Efficient-Differentially-Private-Fine-Tuning-of-LLMs-via-Reinforcement-Learning--VmlldzoxMzc4NTEwMA?accessToken=qhs4n7sh3o93yql2wprb2vylpmer07r2bjvzft7gty5mhhplt3numljxppfd8z66
**模型功能**：通过强化学习动态调整差分隐私参数，优化大语言模型微调过程中的隐私-效用权衡。

**分析时间**：2025-08-04T19:06:34.436512
