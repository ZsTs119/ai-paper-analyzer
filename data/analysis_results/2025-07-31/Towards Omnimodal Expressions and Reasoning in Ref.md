# Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation

**论文ID**：2507.22886
**英文标题**：Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation
**中文标题**：Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation
**论文地址**：https://arxiv.org/abs/2507.22886

**作者团队**：Kaining Ying, Henghui Ding, Guanquan Jie, Yu-Gang Jiang
**发表日期**：2025-07-30

**英文摘要**：
Referring audio-visual segmentation (RAVS) has recently seen significant
advancements, yet challenges remain in integrating multimodal information and
deeply understanding and reasoning about audiovisual content. To extend the
boundaries of RAVS and facilitate future research in this field, we propose
Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset
containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS
stands out with three key innovations: (1) 8 types of multimodal expressions
that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on
understanding audio content beyond just detecting their presence; and (3) the
inclusion of complex reasoning and world knowledge in expressions. Furthermore,
we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the
challenges of multimodal reasoning and fine-grained understanding of
audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and
perform reasoning-based segmentation. Extensive experiments show that OISA
outperforms existing methods on OmniAVS and achieves competitive results on
other related tasks.

**中文摘要**：
Referring audio-visual segmentation (RAVS) has recently seen significant
advancements, yet challenges remain in integrating multimodal information and
deeply understanding and reasoning about audiovis...

**GitHub仓库**：暂无
**项目页面**：暂无
**模型功能**：暂无

**分析时间**：2025-08-04T19:06:26.005440
